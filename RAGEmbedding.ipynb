{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9a2545-e696-4256-935f-e96a02480373",
   "metadata": {},
   "source": [
    "# <center>Panongbene Sawadogo</center>\n",
    "\n",
    "📩 **Contact** : amet1900@gmail.com\n",
    "\n",
    "🌐 **Linkedin** : https://www.linkedin.com/in/panongbene-jean-mohamed-sawadogo-33234a168/\n",
    "\n",
    "🗓️ **Dernière modification** : 20 August 2025\n",
    "\n",
    "# <center>**Embedding chunks for Retrieval-Augmented Generation (RAG)**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a900e76-20c7-4cc7-8ff0-b996b8abea65",
   "metadata": {},
   "source": [
    "In this document, I implement and compare different techniques for **embedding text chunks**, in an optimal way, to build an effective **Retrieval-Augmented Generation (RAG)** system.\n",
    "\n",
    "**Embedding** involves representing a text (word, sentence, paragraph, or chunk) as a numerical vector in a high-dimensional space. These representations enable the measurement of semantic similarity between texts and are essential in a **Retrieval-Augmented Generation (RAG)** system.\n",
    "\n",
    "The goal is to explore how to transform these text units into vector representations (embeddings) that can then be **compared, indexed, and leveraged** efficiently within a RAG pipeline.\n",
    "\n",
    "More specifically, this work aims to:\n",
    "\n",
    "1. **Evaluate different embedding methods** (based on general-purpose models or models specialized in semantic similarity).\n",
    "2. **Analyze the quality of vector representations** to ensure that semantically related chunks are correctly aligned in the vector space.\n",
    "4. **Improve RAG accuracy** by ensuring that the generation model receives context that is both relevant and coherent with the user’s query.\n",
    "5. **Compare the strengths and limitations** of different embedding approaches in terms of quality, performance, and computational cost.\n",
    "\n",
    "In short, this document highlights the crucial importance of the embedding step in a RAG system: it directly determines the model’s ability to retrieve the most relevant information and to generate reliable, contextualized responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de957d8-2b1b-4ec3-b890-3989c22aa987",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "78c856d1-28d9-4656-9052-258b80d411af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install PyPDF2\n",
    "#!pip install -U gensim\n",
    "#!pip install matplotlib\n",
    "#!pip install bitsandbytes\n",
    "#!pip install InstructorEmbedding\n",
    "#!pip install --upgrade transformers\n",
    "#!pip install tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aa463f7-9d33-40f6-b515-f9ddd533b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import PyPDF2\n",
    "import warnings\n",
    "import requests\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from rich.panel import Panel\n",
    "import tensorflow_hub as hub\n",
    "from rich.syntax import Syntax\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.console import Console\n",
    "from dataclasses import dataclass\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "from IPython.display import Markdown, display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple, Union, Callable, Optional\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor, AutoModel, TextStreamer, CLIPModel, LlavaForConditionalGeneration, TextIteratorStreamer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d48d54-5067-44ec-8256-272d33ef97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45667854-abe4-42c3-91f2-18d315d1cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if not already done (only once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fa74a-ad15-4516-be86-c01f74bc814e",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648f9b0a-69a4-4a5d-8bb2-f959c6a4b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraire_texte_pdf(chemin_pdf):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(chemin_pdf):\n",
    "        raise FileNotFoundError(f\"Le fichier {chemin_pdf} n'existe pas.\")\n",
    "    \n",
    "    # Check the file extension\n",
    "    if not chemin_pdf.lower().endswith('.pdf'):\n",
    "        raise ValueError(\"Le fichier doit avoir l'extension .pdf\")\n",
    "    \n",
    "    result_extract = []\n",
    "\n",
    "    with open(chemin_pdf, 'rb') as fichier:\n",
    "        # Create a PdfReader object\n",
    "        lecteur_pdf = PyPDF2.PdfReader(fichier)\n",
    "        \n",
    "        # Get the number of pages\n",
    "        nombre_pages = len(lecteur_pdf.pages)\n",
    "        print(f\"Nombre de pages dans le PDF: {nombre_pages}\")\n",
    "        \n",
    "        # Extract the text from each page\n",
    "        for numero_page in range(nombre_pages):\n",
    "            page = lecteur_pdf.pages[numero_page]\n",
    "            result_extract.append(page.extract_text().strip())\n",
    "\n",
    "    return result_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9af0a5bd-397e-404f-a95b-c5711a252a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de pages dans le PDF: 135\n",
      "CPU times: user 2.27 s, sys: 38.6 ms, total: 2.31 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Extract the text\n",
    "texte_extrait = extraire_texte_pdf('docs/snd.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ad2a8-2d48-49e0-a1b0-5bdb2a0f9cfc",
   "metadata": {},
   "source": [
    "# Sentence or Paragraph Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a223a89c-feb7-4375-a67a-8c8ae6bebd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmenter_texte(text: str, mode: str = \"phrases\", chunk_size: int = 1, language: str = \"french\" ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Segments a text into chunks of sentences or paragraphs.\n",
    "    Parameters:\n",
    "        text: Text to segment\n",
    "        mode: \"phrases\" or \"paragraphes\"\n",
    "        chunk_size: Number of elements per chunk\n",
    "        language: Language used for sentence tokenization\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    Raises:\n",
    "        ValueError: If parameters are invalid\n",
    "    \"\"\"\n",
    "    # Validation of entries\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        raise ValueError(\"Text must not be empty\")\n",
    "    \n",
    "    if mode not in {\"phrases\", \"paragraphes\"}:\n",
    "        raise ValueError(\"Mode must be 'sentences' or 'paragraphs'\")\n",
    "    \n",
    "    if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
    "        raise ValueError(\"The chunk size must be a positive integer\")\n",
    "\n",
    "    # Text cleaning\n",
    "    text = text.strip()\n",
    "    \n",
    "    if mode == \"phrases\":\n",
    "        # Sentence Segmentation with NLTK\n",
    "        elements = sent_tokenize(text, language=language)\n",
    "    else:\n",
    "        # Paragraph segmentation\n",
    "        elements = [p for p in text.split('\\n') if p.strip()]\n",
    "    \n",
    "    # Creating chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(elements), chunk_size):\n",
    "        chunk = elements[i:i + chunk_size]\n",
    "        \n",
    "        # Join with space for sentences, line break for paragraphs\n",
    "        separator = \" \" if mode == \"phrases\" else \"\\n\"\n",
    "        chunks.append(separator.join(chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "497e9250-7e1c-4f42-968c-cf391a5495de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.8 ms, sys: 3.17 ms, total: 29 ms\n",
      "Wall time: 29.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "paragraph_segments = []\n",
    "for one_page in texte_extrait:\n",
    "    paragraph_segments+=segmenter_texte(one_page, \"phrases\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "839ac2b8-ed7a-40f0-b644-034ba9095380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "778"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1df49a79-8458-4058-8a1d-3288b176ce4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Face à cette situation, le Sénégal demeure parmi le s 25 pays à plus faible \\ndéveloppement humain, avec un IDH de 0,517 en 2022. Dans le même ordre \\nd’idées, l’indice du capital humain du Sénégal est resté relativement faible \\n(0,42), comparé à des pays comme la Malaisie (0,61)  ou le Ghana (0,504).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_segments[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b101d-2b5e-4dc1-b2c1-e8b259e1e720",
   "metadata": {},
   "source": [
    "# Word Embeddings (word level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab920aee-9f8c-4672-aa5e-919d0aaa15e6",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Word embeddings at the word level consist of representing each word in a text as a numerical vector in a high-dimensional space, where the proximity between vectors reflects the semantic or contextual similarity between words. This technique relies on machine learning models such as **Word2Vec** (CBOW or Skip-gram), **GloVe**, or contextual embeddings from models like **BERT**, which are trained on large text corpora. During training, words that appear in similar contexts (for example, “dog” and “cat” in animal-related sentences) are projected close to each other in the vector space. For instance, in a Word2Vec embedding, the vector for “king” may be close to that of “queen” in terms of direction, reflecting a semantic relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Capturing semantic similarity**: Word embeddings model semantic relationships between words (synonymy, antonymy, analogy), such as “man” is to “king” what “woman” is to “queen,” which facilitates applications like translation or text generation.\n",
    "2. **Dimensionality reduction**: By transforming words into vectors (typically 50–300 dimensions), this method compresses information compared to a one-hot representation (where each word is a binary vector equal to the vocabulary size), improving computational efficiency.\n",
    "4. **Generalization across contexts**: Once trained on a diverse corpus, embeddings can be applied to new texts without retraining, offering flexibility across domains.\n",
    "5. **Partial interpretability**: Arithmetic relationships between vectors (e.g., vector(“king”) – vector(“man”) + vector(“woman”) ≈ vector(“queen”)) allow qualitative analysis of semantic similarities.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Lack of dynamic context**: Static embeddings (like Word2Vec or GloVe) assign a single vector per word, ignoring variations in meaning depending on context (e.g., “bank” can mean a financial institution or a riverbank), which limits precision in ambiguous texts.\n",
    "2. **Dependence on training corpus quality**: Embeddings reflect the biases and gaps of the text they are trained on. For example, a corpus lacking linguistic diversity may produce less relevant vectors for certain words or domains.\n",
    "3. **High initial training cost**: Training embeddings on large corpora requires significant resources (time, compute power), making it costly, although pre-trained models mitigate this issue.\n",
    "4. **Out-of-vocabulary (OOV) problem**: Words missing from the training vocabulary (e.g., neologisms or domain-specific terms) are not represented, forcing the use of techniques like subword embeddings (e.g., FastText) or ignoring these words.\n",
    "5. **Limitations with complex relations**: While capable of capturing similarities, word-level embeddings struggle to model syntactic relations or long-range dependencies, which require contextual approaches such as transformers.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Use contextual embeddings**: Move to models like **BERT** or **ELMo**, which generate dynamic embeddings based on context, overcoming the limitations of static embeddings.\n",
    "* **Domain-specific training**: Adapt embeddings to a specialized domain (e.g., medical or legal) via fine-tuning to improve relevance.\n",
    "* **Handling OOV words**: Incorporate subword techniques (such as **BPE** or **WordPiece**) to represent unknown words using subunits.\n",
    "* **Resource optimization**: Leverage pre-trained embeddings (available via Hugging Face or TensorFlow Hub) to reduce training costs.\n",
    "* **Evaluation**: Assess embeddings with metrics like **cosine similarity** or benchmarks like **WordSim-353** to validate their semantic quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c34a0-cc16-47d8-b8a0-9850776ecf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeddings(embedding_type='word2vec', embedding_path=None):\n",
    "    \"\"\"\n",
    "    Load pre-trained word embeddings into a Gensim KeyedVectors model.\n",
    "\n",
    "    This function supports multiple embedding formats such as Word2Vec, \n",
    "    GloVe, and FastText. It uses Gensim's `KeyedVectors` to load the \n",
    "    embeddings from the specified file path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding_type : str, optional (default='word2vec')\n",
    "        The type of embeddings to load. Supported values are:\n",
    "        - 'word2vec' : Loads Word2Vec binary format embeddings.\n",
    "        - 'glove'    : Loads GloVe embeddings converted to Word2Vec format \n",
    "                       (without headers).\n",
    "        - 'fasttext' : Loads FastText embeddings in text format.\n",
    "\n",
    "    embedding_path : str\n",
    "        Path to the embeddings file to be loaded.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : gensim.models.KeyedVectors\n",
    "        The loaded embedding model, which allows vector lookup, similarity \n",
    "        computations, and other operations.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If the provided embedding_type is not supported.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> model = load_word_embeddings('word2vec', 'GoogleNews-vectors-negative300.bin')\n",
    "    >>> vector = model['king']  # Get embedding vector for the word 'king'\n",
    "    >>> model.most_similar('paris')\n",
    "    \"\"\"\n",
    "\n",
    "    if embedding_type.lower() == 'word2vec':\n",
    "        # Load Word2Vec binary embeddings\n",
    "        model = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "    elif embedding_type.lower() == 'glove':\n",
    "        # Load GloVe embeddings (converted to Word2Vec format without header)\n",
    "        model = KeyedVectors.load_word2vec_format(embedding_path, binary=False, no_header=True)\n",
    "    elif embedding_type.lower() == 'fasttext':\n",
    "        # Load FastText embeddings (in text format)\n",
    "        model = KeyedVectors.load_word2vec_format(embedding_path, binary=False)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embedding type. Choose 'word2vec', 'glove', or 'fasttext'.\")\n",
    "\n",
    "    print(f\"{embedding_type} model loaded successfully!\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce78eca-0ac0-456a-b6e1-02cfb105ebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_embeddings(embedding_type='word2vec', embedding_path=None):\n",
    "    \"\"\"\n",
    "    Load pre-trained word embeddings into a Gensim KeyedVectors model.\n",
    "\n",
    "    This function supports multiple embedding formats such as Word2Vec, \n",
    "    GloVe, and FastText. It uses Gensim's `KeyedVectors` to load the \n",
    "    embeddings from the specified file path.\n",
    "    \n",
    "    Parameters\n",
    "        embedding_type : (default='word2vec') The type of embeddings to load. Supported values are:\n",
    "            - 'word2vec' : Loads Word2Vec binary format embeddings.\n",
    "            - 'glove'    : Loads GloVe embeddings converted to Word2Vec format \n",
    "                           (without headers).\n",
    "            - 'fasttext' : Loads FastText embeddings in text format.\n",
    "        embedding_path : Path to the embeddings file to be loaded.\n",
    "\n",
    "    Returns\n",
    "        model : gensim.models.KeyedVectors\n",
    "            The loaded embedding model, which allows vector lookup, similarity \n",
    "            computations, and other operations.\n",
    "        Raises ValueError If the provided embedding_type is not supported.\n",
    "\n",
    "    Examples\n",
    "        >>> model = load_word_embeddings('word2vec', 'GoogleNews-vectors-negative300.bin')\n",
    "        >>> vector = model['king']  # Get embedding vector for the word 'king'\n",
    "        >>> model.most_similar('paris')\n",
    "    \"\"\"    \n",
    "    if embedding_type.lower() == 'word2vec':\n",
    "        # Load Word2Vec binary embeddings\n",
    "        model = KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "    elif embedding_type.lower() == 'glove':\n",
    "        # Load GloVe embeddings (converted to Word2Vec format without header)\n",
    "        model = KeyedVectors.load_word2vec_format(embedding_path, binary=False, no_header=True)\n",
    "    elif embedding_type.lower() == 'fasttext':\n",
    "        # Load FastText embeddings (in text format\n",
    "        model = KeyedVectors.load_word2vec_format(embedding_path, binary=False)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embedding type. Choose 'word2vec', 'glove', or 'fasttext'.\")\n",
    "    \n",
    "    print(f\"Modèle {embedding_type} chargé avec succès !\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404eb7af-8fe0-4e5b-853f-fd73f4769f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(model, word):\n",
    "    \"\"\"\n",
    "    Retrieve the embedding vector for a given word from a pre-trained model.\n",
    "\n",
    "    This function takes a loaded embedding model (e.g., Word2Vec, GloVe, \n",
    "    or FastText via Gensim KeyedVectors) and returns the vector representation \n",
    "    of the specified word. If the word is not present in the model's vocabulary, \n",
    "    it will handle the error gracefully.\n",
    "\n",
    "    Parameters\n",
    "        model : gensim.models.KeyedVectors\n",
    "            The pre-trained embedding model loaded with Gensim.\n",
    "        \n",
    "        word : str\n",
    "            The word for which the embedding vector should be retrieved.\n",
    "\n",
    "    Returns\n",
    "        numpy.ndarray or None\n",
    "            A vector (1D NumPy array) representing the word's embedding \n",
    "            if the word exists in the model. Returns `None` if the word \n",
    "            is not found in the vocabulary.\n",
    "\n",
    "    Raises \n",
    "        KeyError\n",
    "            If the word is not in the model's vocabulary (caught internally \n",
    "            and handled with a warning).\n",
    "\n",
    "    Examples\n",
    "        >>> model = load_word_embeddings('word2vec', 'GoogleNews-vectors-negative300.bin')\n",
    "        >>> vector = get_word_embedding(model, 'king')\n",
    "        >>> print(vector.shape)\n",
    "        (300,)\n",
    "        >>> get_word_embedding(model, 'unknownword')\n",
    "        None\n",
    "    \"\"\"\n",
    "    if word in model:\n",
    "        return model[word]\n",
    "    else:\n",
    "        print(f\"The word '{word}' is not in the vocabulary.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44146266-9896-4c99-a9f5-963db2c623d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the path to your embeddings file\n",
    "embedding_path = \"GoogleNews-vectors-negative300.bin\"  # Ex: Word2Vec\n",
    "#embedding_path = \"glove.6B.300d.txt\"  # Ex: GloVe\n",
    "#embedding_path = \"wiki-news-300d-1M.vec\"  # Ex: FastText\n",
    "\n",
    "# Load the model\n",
    "model = load_word_embeddings(embedding_type='word2vec', embedding_path=embedding_path)\n",
    "\n",
    "if model:\n",
    "    # Get the embedding of a word\n",
    "    word = \"king\"\n",
    "    embedding = get_word_embedding(model, word)\n",
    "    \n",
    "    if embedding is not None:\n",
    "        print(f\"Warning: The word '{word}':\\n{embedding[:5]}... (shape: {embedding.shape})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931fad70-6169-4677-ad54-d312b3174e08",
   "metadata": {},
   "source": [
    "# Contextual Embeddings (sentence / chunk level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38227de-04e7-4ae6-bc98-e95bc4977298",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Contextual embeddings at the sentence or chunk level represent a word, sentence, or segment of text (chunk) as a numerical vector in a high-dimensional space, while taking into account the context in which they appear. Unlike static word embeddings (such as Word2Vec), these representations are generated dynamically by transformer-based models such as BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly Optimized BERT Approach), or DistilBERT (a lightweight version of BERT). These models analyze text bidirectionally and use attention mechanisms to capture relationships between words in a sequence. For example, in the sentence *“The bank financed the project”*, the word *“bank”* will have a different embedding depending on whether it refers to a financial institution or the shore of a river, reflecting its contextual meaning. These embeddings are particularly suitable for chunks, sentences, or short documents, as they exploit local structure to produce rich representations, often extracted from the intermediate or final layers of the models after fine-tuning or direct inference.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Capture polysemy and contextual semantics**: Thanks to their bidirectional and contextual nature, these embeddings distinguish between different meanings of a word depending on its environment. For example, in *“I’m going to the bank to fish”* vs *“I’m going to the bank to borrow”*, the vectors for *“bank”* will be distinct, improving accuracy in tasks such as word sense disambiguation or semantic search.\n",
    "2. **Well-suited for chunks, sentences, and short documents**: Models like BERT or DistilBERT are optimized to process moderately long sequences (up to 512 tokens for standard BERT), making them ideal for segmenting and encoding textual units such as sentences or chunks in RAG pipelines, where local context is crucial.\n",
    "3. **High performance in advanced NLP tasks**: These embeddings excel in applications such as text classification, question answering (QA), or text generation, as they incorporate complex syntactic and semantic relationships captured by attention mechanisms.\n",
    "4. **Reuse of pre-trained models**: Thanks to libraries such as Hugging Face Transformers, contextual embeddings can be quickly obtained from pre-trained models, reducing the need for custom training and accelerating development.\n",
    "5. **Flexibility with fine-tuning**: These models can be adapted to specific corpora (e.g., medical or legal) to improve their relevance in specialized domains, enabling customization without training from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Often high-dimensional embeddings**: The generated vectors (typically 768 dimensions for BERT or 512 for DistilBERT) are memory-intensive, which can be problematic for indexing large corpora or running on low-power devices like mobile phones.\n",
    "2. **High computational cost for large corpora**: Generating embeddings for each chunk or sentence requires intensive computation, especially on large texts or massive datasets, making this method costly in terms of time and resources (CPU/GPU), even with lighter models like DistilBERT.\n",
    "3. **Sequence length limitations**: Models like BERT are limited to 512 tokens (or 1024 for some variants), which restricts their application to long documents. Chunks exceeding this limit must be truncated or split, potentially losing context.\n",
    "4. **Dependence on training data quality**: The embeddings reflect the biases or gaps in the corpora on which the models were trained (e.g., lack of linguistic diversity), which may affect performance on out-of-domain texts.\n",
    "5. **Implementation complexity**: While pre-trained models ease usage, integrating these embeddings into a RAG pipeline or application requires careful resource management (batch optimization, memory handling) and expertise in fine-tuning to maximize efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Use lighter models**: Choose DistilBERT or other optimized versions (like TinyBERT) to reduce embedding dimensionality and computational cost while maintaining good performance.\n",
    "* **Handle long sequences**: Implement techniques such as sliding windows or smart truncation with overlap to process documents exceeding token limits while preserving context.\n",
    "* **Optimize resources**: Use frameworks like ONNX or TensorRT to speed up inference on large corpora, or leverage parallel GPU computation.\n",
    "* **Domain-specific fine-tuning**: Adapt the model on targeted corpora to reduce biases and improve embedding relevance in specific fields.\n",
    "* **Evaluation**: Benchmark embeddings with tasks like GLUE or contextual similarity metrics to validate their effectiveness in specific tasks such as RAG.\n",
    "\n",
    "---\n",
    "\n",
    "This technique requires proper infrastructure and careful configuration for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81baa8c3-c83c-45eb-b0c3-fd3f87bb12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_contextual_embeddings(model_name='bert-base-uncased'):\n",
    "    \"\"\"\n",
    "    Load a pre-trained transformer model and its tokenizer to generate contextual embeddings.\n",
    "\n",
    "    This function initializes a Hugging Face transformer model (e.g., BERT, RoBERTa, DistilBERT) \n",
    "    along with its tokenizer. The model can then be used to compute contextual embeddings for words, \n",
    "    sentences, or chunks of text. Unlike static embeddings (e.g., Word2Vec), contextual embeddings \n",
    "    are dynamically generated depending on the surrounding context.\n",
    "\n",
    "    Parameters:\n",
    "        model_name (str, optional): The name or path of the pre-trained model from the Hugging Face Hub. \n",
    "            Defaults to `'bert-base-uncased'`. \n",
    "            Examples include:\n",
    "                - 'bert-base-uncased'\n",
    "                - 'roberta-base'\n",
    "                - 'distilbert-base-uncased'\n",
    "                - or a custom fine-tuned model path.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - tokenizer (PreTrainedTokenizer): The tokenizer associated with the model, \n",
    "              used to preprocess input text.\n",
    "            - model (PreTrainedModel): The transformer model loaded for generating embeddings.\n",
    "\n",
    "    Example:\n",
    "        >>> tokenizer, model = load_contextual_embeddings(\"bert-base-uncased\")\n",
    "        >>> inputs = tokenizer(\"The bank financed the project\", return_tensors=\"pt\")\n",
    "        >>> outputs = model(**inputs)\n",
    "        >>> embeddings = outputs.last_hidden_state\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    print(f\"Model {model_name} loaded successfully!\")\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ed16cfd-b045-40aa-ab3e-470877aead2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_embedding(tokenizer, model, text, pooling_strategy='mean'):\n",
    "    \"\"\"\n",
    "    Generate a contextual embedding for a given text using a transformer model.\n",
    "    Parameters\n",
    "        tokenizer : PreTrainedTokenizer\n",
    "            The tokenizer corresponding to the model (e.g., from Hugging Face Transformers).\n",
    "        model : PreTrainedModel\n",
    "            The transformer model (e.g., BERT, RoBERTa) used to generate contextual embeddings.\n",
    "        text : str\n",
    "            The input text (sentence, chunk, or document) for which the embedding will be computed.\n",
    "        pooling_strategy : str, optional, default='mean'\n",
    "            The strategy to pool token embeddings into a single vector representation:\n",
    "                - 'mean' : average pooling of all token embeddings.\n",
    "                - 'cls'  : use the [CLS] token embedding.\n",
    "                - 'max'  : max pooling across token embeddings.\n",
    "    \n",
    "        Returns\n",
    "        numpy.ndarray\n",
    "            A 1D vector representing the contextual embedding of the input text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenisation\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Retrieving hidden states (last layer)\n",
    "    last_hidden_states = outputs.last_hidden_state.squeeze(0)\n",
    "    \n",
    "    # Pooling\n",
    "    if pooling_strategy == 'mean':\n",
    "        embedding = torch.mean(last_hidden_states, dim=0)\n",
    "    elif pooling_strategy == 'max':\n",
    "        embedding, _ = torch.max(last_hidden_states, dim=0)\n",
    "    elif pooling_strategy == 'cls':\n",
    "        embedding = last_hidden_states[0]  # Token [CLS]\n",
    "    else:\n",
    "        raise ValueError(\"Stratégie de pooling non supportée. Choisissez 'mean', 'max' ou 'cls'.\")\n",
    "    \n",
    "    return embedding.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca0bddc2-b0b6-4718-a5d3-0a9b48fb55eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle bert-base-uncased chargé avec succès !\n",
      "Embedding contextuel pour le texte 'Face à cette situation, le Sénégal demeure parmi le s 25 pays à plus faible \n",
      "développement humain, avec un IDH de 0,517 en 2022. Dans le même ordre \n",
      "d’idées, l’indice du capital humain du Sénégal est resté relativement faible \n",
      "(0,42), comparé à des pays comme la Malaisie (0,61)  ou le Ghana (0,504).':\n",
      "Shape: (768,)\n",
      "Premiers éléments: [-0.45379516  0.24887244 -0.10547437 -0.28441253  0.2409869 ]...\n"
     ]
    }
   ],
   "source": [
    "# Model selection (eg: 'bert-base-uncased', 'roberta-base', 'distilbert-base-uncased')\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "# Loading the model\n",
    "tokenizer, model = load_contextual_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc4caa2b-8754-4700-9bed-a93f8e7c7306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de paragraph :  778\n",
      "Embedding contextuel pour le texte 'Face à cette situation, le Sénégal demeure parmi le s 25 pays à plus faible \n",
      "développement humain, avec un IDH de 0,517 en 2022. Dans le même ordre \n",
      "d’idées, l’indice du capital humain du Sénégal est resté relativement faible \n",
      "(0,42), comparé à des pays comme la Malaisie (0,61)  ou le Ghana (0,504).':\n",
      "Shape: (768,)\n",
      "Premiers éléments: [-0.49004218 -0.07687725 -0.17948622 -0.17795789  0.31216305]...\n",
      "CPU times: user 2min 49s, sys: 1min 25s, total: 4min 14s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Text to encode\n",
    "paragraph_segments_embbeding = []\n",
    "\n",
    "print('Number of paragraphs : ', len(paragraph_segments))\n",
    "# Embedding generation\n",
    "for one_para in paragraph_segments:\n",
    "    embedding = get_contextual_embedding(tokenizer, model, one_para, pooling_strategy='mean')\n",
    "    paragraph_segments_embbeding.append(embedding)\n",
    "\n",
    "if embedding is not None:\n",
    "    print(f\"Contextual embedding for texte '{text}':\")\n",
    "    print(f\"Shape: {embedding.shape}\")\n",
    "    print(f\"First elements: {embedding[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b238fd24-244a-4099-99b5-856c891ecae7",
   "metadata": {},
   "source": [
    "# Sentence Embeddings (optimized for sentence similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3437a-eec1-4022-8d98-3a81519fae30",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Sentence embeddings, optimized for sentence similarity, consist of generating compact numerical vectors that represent sentences, paragraphs, or entire chunks while capturing their overall semantic meaning. Unlike word embeddings or contextual embeddings, which focus on individual words or their local context, these models—such as Sentence-BERT (SBERT), LaBSE (Language-agnostic BERT Sentence Embeddings), or the Universal Sentence Encoder—are specifically designed to encode larger textual units. These models rely on transformer architectures (often derived from BERT) fine-tuned with sentence pairs and tasks such as similarity classification (e.g., Natural Language Inference or STS – Semantic Textual Similarity). For example, SBERT applies a pooling technique (mean or max) over word embeddings to produce a single vector per sentence, so that sentences like “The cat is sleeping” and “The feline is resting” would be close in semantic similarity. These embeddings are especially useful in RAG pipelines, where they facilitate retrieving relevant chunks by measuring vector proximity.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Optimized for semantic similarity and retrieval**: These embeddings are trained to maximize the correlation between vector distance and semantic similarity, making models like SBERT ideal for tasks such as similar text retrieval or paraphrase detection. For instance, “What is the weather today?” and “What’s the current forecast?” will produce close vectors.\n",
    "2. **Directly suited for RAG**: By producing compact representations (often 384 or 768 dimensions) for entire chunks or sentences, these embeddings allow efficient indexing and fast retrieval in RAG systems, improving the relevance of generated answers.\n",
    "3. **Multilingual performance**: Models like LaBSE are designed to work across multiple languages, offering a robust solution for multilingual corpora without the need for retraining per language.\n",
    "4. **Relative computational efficiency**: Compared to contextual embeddings applied at each token, sentence embeddings reduce computational overhead by generating a single vector per text unit, which is advantageous for real-time applications.\n",
    "5. **Easy reuse**: Thanks to frameworks like Hugging Face, pre-trained models (e.g., SBERT’s `all-mpnet-base-v2`) are directly accessible, allowing quick integration into NLP pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Quality depends on training corpus**: The performance of embeddings is strongly influenced by the diversity and representativeness of the fine-tuning corpus. For example, a model trained on formal English text may underperform on informal dialogues or low-resource languages.\n",
    "2. **Less effective on very long texts**: These models are optimized for short to moderately long sequences (generally up to 512 tokens for SBERT). Their ability to capture context decreases with long paragraphs or documents, requiring prior segmentation or truncation that may lead to information loss.\n",
    "3. **Sensitivity to model biases**: Embeddings reflect biases present in the training data (e.g., gender stereotypes or cultural gaps), which can impact accuracy in sensitive or diverse contexts.\n",
    "4. **Dependence on fine-tuning**: Although pre-trained, these models often require adjustment for domain-specific tasks (e.g., similarity in the medical domain) to achieve full potential, adding a configuration step.\n",
    "5. **Limitations with complex relations**: While excellent for similarity, these embeddings may lack granularity to capture long-distance syntactic dependencies or subtle nuances, which can be a drawback for highly detailed analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Domain adaptation**: Fine-tune on a specific corpus (e.g., medical or legal) to improve embedding quality in a given context.\n",
    "* **Handling long texts**: Combine with hierarchical segmentation or sliding windows to process large documents, applying embeddings to smaller sub-units.\n",
    "* **Bias reduction**: Use debiasing techniques (e.g., Bolukbasi et al. methods) to mitigate stereotypes in embeddings.\n",
    "* **Resource optimization**: Leverage lighter versions (e.g., SBERT’s `miniLM`) or parallel computation to reduce computational cost on large corpora.\n",
    "* **Evaluation**: Validate embeddings with benchmarks such as STS Benchmark or cosine similarity metrics in a RAG pipeline to assess effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61ecf650-619d-44a2-8400-350083069ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_embedding_model(model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"\n",
    "    Load a pre-trained sentence embedding model.\n",
    "\n",
    "    This function loads a sentence embedding model from the \n",
    "    <<sentence_transformers>> library. Such models are optimized \n",
    "    for generating dense vector representations of entire \n",
    "    sentences, paragraphs, or chunks, making them particularly \n",
    "    useful for semantic similarity, clustering, and RAG pipelines.\n",
    "\n",
    "    Parameters\n",
    "        model_name : str, optional (default='all-MiniLM-L6-v2')\n",
    "            The name or path of the pre-trained sentence embedding model\n",
    "            to load. Examples include:\n",
    "            - 'all-MiniLM-L6-v2' (lightweight and efficient)\n",
    "            - 'all-mpnet-base-v2' (higher accuracy, larger model)\n",
    "            - 'LaBSE' (multilingual performance)\n",
    "\n",
    "    Returns\n",
    "        model : SentenceTransformer\n",
    "            A SentenceTransformer model instance that can be used to \n",
    "            encode sentences or chunks into embeddings.\n",
    "\n",
    "    Example\n",
    "        >>> model = load_sentence_embedding_model('all-MiniLM-L6-v2')\n",
    "        >>> embeddings = model.encode([\"The cat is sleeping\", \"The feline rests\"])\n",
    "        >>> print(embeddings.shape)  # (2, 384)\n",
    "    \"\"\"\n",
    "    if model_name.lower() in ['universal-sentence-encoder', 'universal-sentence-encoder-multilingual']:\n",
    "        # Loading via TensorFlow Hub\n",
    "        model = hub.load(f\"https://tfhub.dev/google/{model_name}/4\")\n",
    "        print(f\"Model {model_name} loaded via TF Hub!\")\n",
    "    else:\n",
    "        # Loading via SentenceTransformers (SBERT/LaBSE)\n",
    "        model = SentenceTransformer(model_name)\n",
    "        print(f\"Model {model_name} loaded via SentenceTransformers!\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1a97ce7-728d-41d8-96cd-5c195a3a95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(model, sentences, model_type='sbert'):\n",
    "    \"\"\"\n",
    "    Encode a list of sentences into embeddings using a specified model.\n",
    "\n",
    "    This function converts input sentences into dense vector \n",
    "    representations (embeddings) depending on the type of model provided. \n",
    "    These embeddings capture semantic meaning and can be used for \n",
    "    similarity search, clustering, or downstream NLP tasks.\n",
    "\n",
    "    Parameters\n",
    "        model : object\n",
    "            The embedding model instance to use. This can be:\n",
    "            - A SentenceTransformer model (if `model_type='sbert'`)\n",
    "            - A TensorFlow Hub model such as USE (if `model_type='use'`)\n",
    "            - Any compatible embedding model.\n",
    "        \n",
    "        sentences : list of str\n",
    "            A list of sentences or short texts to encode into embeddings.\n",
    "    \n",
    "        model_type : str, optional (default='sbert')\n",
    "            The type of embedding model. Supported values:\n",
    "            - 'sbert' : for models loaded via Sentence-BERT (SentenceTransformer)\n",
    "            - 'use'   : for Universal Sentence Encoder models\n",
    "            - (other custom options can be added as needed)\n",
    "\n",
    "    Returns\n",
    "        embeddings : numpy.ndarray\n",
    "            A 2D array of shape (n_sentences, embedding_dim) containing \n",
    "            the embeddings for each input sentence.\n",
    "\n",
    "    Example\n",
    "        >>> model = load_sentence_embedding_model('all-MiniLM-L6-v2')\n",
    "        >>> sentences = [\"The cat is sleeping\", \"The feline rests\"]\n",
    "        >>> embeddings = encode_sentences(model, sentences, model_type='sbert')\n",
    "        >>> print(embeddings.shape)  # (2, 384)\n",
    "    \"\"\"\n",
    "    if isinstance(sentences, str):\n",
    "        sentences = [sentences]\n",
    "    \n",
    "    try:\n",
    "        if model_type in ['sbert', 'labse']:\n",
    "            embeddings = model.encode(sentences, convert_to_numpy=True)\n",
    "        elif model_type == 'use':\n",
    "            embeddings = model(sentences).numpy()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type\")\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur d'encodage: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58d3f89e-df0a-443b-8b58-a64afbe9e115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sentence-BERT ===\n",
      "Modèle all-MiniLM-L6-v2 chargé via SentenceTransformers!\n",
      "Shape des embeddings: (778, 384)\n",
      "Similarité phrase 1-2: 1.00\n",
      "Similarité phrase 1-3: 0.70\n",
      "CPU times: user 963 ms, sys: 954 ms, total: 1.92 s\n",
      "Wall time: 5.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\n=== Sentence-BERT ===\")\n",
    "sbert_model = load_sentence_embedding_model('all-MiniLM-L6-v2')\n",
    "if sbert_model:\n",
    "    embeddings = encode_sentences(sbert_model, paragraph_segments, 'sbert')\n",
    "    print(f\"Shape des embeddings: {embeddings.shape}\")  # (3, 384)\n",
    "    print(f\"Similarité phrase 1-2: {np.dot(embeddings[0], embeddings[1]):.2f}\")\n",
    "    print(f\"Similarité phrase 1-3: {np.dot(embeddings[0], embeddings[2]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84d7e6-5971-4d85-b5b2-940bb12e42fe",
   "metadata": {},
   "source": [
    "# Instruction-tuned & Domain-specific Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f80f2-0f6b-470e-8283-b3ad2be8b317",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Instruction-tuned & domain-specific embeddings are vector representations generated by models trained with **explicit instructions** to optimize performance on specific tasks such as semantic search, clustering, classification, or text retrieval in RAG systems. Unlike generic embeddings (like Word2Vec or BERT), these models—such as OpenAI text-embedding-3-small/large, E5 (Efficient Text Embedding), Instructor-XL, GTE (General Text Embedder), or Cohere Embed—are fine-tuned on datasets annotated with instructions or text–text pairs, often accompanied by contextual metadata. For example, a model like Instructor-XL may be trained with instructions such as “Find the documents most similar to this query” or “Classify this text by topic,” guiding the embeddings toward practical tasks. Moreover, these models can be specialized for specific domains (biomedical, legal, multilingual) using targeted corpora, enabling fine adaptation to particular application contexts such as medical knowledge bases or multilingual legal documents.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **More robust and adapted to practical RAG use cases**: These embeddings are designed to maximize relevance in real-world scenarios, such as retrieving relevant chunks in a RAG pipeline. For instance, OpenAI text-embedding-3 is optimized for search tasks, producing vectors that improve the accuracy of generated responses compared to generic embeddings.\n",
    "2. **Support for specialized embeddings**: Models can be fine-tuned for specific domains, such as E5 for biomedical (handling terms like “oncology” or “DNA”) or LaBSE for multilingual use (supporting English, Spanish, Chinese, etc.), allowing customization according to application needs and improving performance on specialized corpora.\n",
    "3. **Effectiveness in instruction-driven tasks**: Thanks to training with explicit instructions, these embeddings better capture the intent behind queries or classifications, making models like GTE particularly useful for intentional searches or thematic clustering.\n",
    "4. **Reusability and scalability**: Pretrained models (available through APIs such as OpenAI or Cohere, or libraries like SentenceTransformers) enable rapid integration into pipelines while supporting large-scale corpora with consistent performance.\n",
    "5. **Improved contextual robustness**: Instruction-based training helps reduce semantic ambiguities and better handle linguistic variations, offering greater stability across diverse environments.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Strong dependence on training data quality**: Embedding performance depends on the representativeness and quality of the datasets used for fine-tuning. For instance, a model trained only on academic texts may underperform on informal dialogues or uncovered domains.\n",
    "2. **Potentially high computational cost**: Instruction-based training, especially for large-scale models such as OpenAI text-embedding-3-large or Instructor-XL, requires significant resources (GPU, training time), and inference itself can be costly on large corpora—though lighter versions (such as text-embedding-3-small) help mitigate this.\n",
    "3. **Risk of over-optimization**: Excessive fine-tuning on a specific task or domain may reduce generalization to other contexts, making the model less versatile if instructions do not cover all possible variations.\n",
    "4. **Complexity of access and maintenance**: Some models (such as those from OpenAI or Cohere) require paid APIs or licenses, and their integration into a RAG pipeline may involve ongoing cost and update management.\n",
    "5. **Limitations with very long or unstructured texts**: While effective on chunks or sentences, these embeddings may lose efficiency with very long documents or texts lacking clear structure, requiring pre-segmentation or truncation that can introduce errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Custom fine-tuning**: Adapt the model to a specific corpus (e.g., legal or multilingual) with tailored instructions to improve domain relevance.\n",
    "* **Resource optimization**: Use lighter versions (like text-embedding-3-small or GTE-base) or apply quantization techniques (e.g., post-training quantization) to reduce computational costs.\n",
    "* **Bias management**: Apply debiasing methods (such as Bolukbasi’s adjustments) to mitigate the effects of biased training data.\n",
    "* **Pre-segmentation**: Combine with techniques such as sliding windows or hierarchical segmentation to handle long texts by applying embeddings to optimized sub-units.\n",
    "* **Evaluation**: Benchmark embeddings using MTEB (Massive Text Embedding Benchmark) or precision metrics within a RAG pipeline to validate effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "This technique requires careful attention to resource use and training data quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "07b3febd-bc15-4a2e-8d84-976e1a482e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionEmbeddings:\n",
    "    def __init__(self, model_name: str = \"e5-small-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding model with instruction (without OpenAI)\n",
    "        Available models:\n",
    "            * **E5**: 'e5-small-v2', 'e5-base-v2', 'e5-large-v2'\n",
    "            * **Instructor**: 'instructor-small', 'instructor-base', 'instructor-xl'\n",
    "            * **GTE**: 'gte-small', 'gte-base'\n",
    "            * **Cohere**: requires an API key but has an open-source alternative\n",
    "        \"\"\"\n",
    "        self.model_name = model_name.lower()\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Loads and initializes the pre-trained model and its tokenizer.\n",
    "\n",
    "        This internal method prepares the model for inference by:\n",
    "        - Loading the model weights from the specified pre-trained checkpoint.\n",
    "        - Initializing the corresponding tokenizer for text processing.\n",
    "        - Configuring device placement (CPU or GPU) and model-specific settings.\n",
    "    \n",
    "        This ensures that the model is ready to generate embeddings or perform other \n",
    "        NLP tasks.\n",
    "    \n",
    "        Returns\n",
    "            None\n",
    "    \n",
    "        Notes\n",
    "            This is a private/internal method intended to be called during class \n",
    "            initialization or when the model needs to be reloaded.\n",
    "        \"\"\"\n",
    "        \n",
    "        if 'e5' in self.model_name:\n",
    "            # E5 models (Embeddings from Bidirectional Encoder Representations)\n",
    "            self.model = SentenceTransformer(f\"intfloat/{self.model_name}\")\n",
    "            print(f\"Loaded E5 model: {self.model_name}\")\n",
    "        \n",
    "        elif 'instructor' in self.model_name:\n",
    "            # Instructor Models (Instruction-tuned)\n",
    "            self.model = INSTRUCTOR(f\"hkunlp/{self.model_name}\")\n",
    "            print(f\"Modèle Instructor chargé: {self.model_name}\")\n",
    "        \n",
    "        elif 'gte' in self.model_name:\n",
    "            # GTE (General Text Embeddings)\n",
    "            self.model = SentenceTransformer(f\"thenlper/{self.model_name}\")\n",
    "            print(f\"Loaded GTE model: {self.model_name}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Modèle non supporté: {self.model_name}. Choisissez parmi E5, Instructor ou GTE.\")\n",
    "\n",
    "\n",
    "    def encode(self, \n",
    "               texts: Union[str, List[str]], \n",
    "               instruction: str = None,\n",
    "               task_type: str = None) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes one or multiple texts into numerical embeddings using a pre-trained model.\n",
    "\n",
    "        This function generates vector representations of input text(s), which capture \n",
    "        semantic meaning and can be used for tasks like semantic search, clustering, \n",
    "        classification, or other NLP applications. Optional instructions or task types \n",
    "        can guide the model to produce embeddings tailored to specific contexts.\n",
    "    \n",
    "        Parameters\n",
    "            texts : str or List[str]\n",
    "                A single text string or a list of text strings to encode.\n",
    "            instruction : str, optional\n",
    "                An optional instruction to guide the embedding generation. For example, \n",
    "                specifying \"summarize\" or \"analyze sentiment\" may influence the resulting embedding.\n",
    "            task_type : str, optional\n",
    "                An optional task type indicator that can modify how embeddings are computed, \n",
    "                depending on the model's capabilities (e.g., \"classification\", \"retrieval\").\n",
    "        \n",
    "            Returns\n",
    "            np.ndarray\n",
    "                A NumPy array of embeddings:\n",
    "                - If `texts` is a single string, returns a 1D array.\n",
    "                - If `texts` is a list, returns a 2D array where each row corresponds to a text.\n",
    "    \n",
    "        Example\n",
    "            >>> embedding = encoder.encode(\"This is a sample text.\")\n",
    "            >>> embedding.shape\n",
    "            (768,)\n",
    "            \n",
    "            >>> embeddings = encoder.encode([\"Text 1\", \"Text 2\"])\n",
    "            >>> embeddings.shape\n",
    "            (2, 768)\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        # E5 Models - Adds task prefix automatically\n",
    "        if 'e5' in self.model_name:\n",
    "            if task_type == 'search':\n",
    "                texts = [f\"query: {text}\" for text in texts]\n",
    "            elif task_type == 'passage':\n",
    "                texts = [f\"passage: {text}\" for text in texts]\n",
    "            return self.model.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        # Instructor Models - Requires explicit instruction\n",
    "        elif 'instructor' in self.model_name:\n",
    "            if instruction is None:\n",
    "                instruction = \"Represent the text for retrieval:\"\n",
    "            paired_inputs = [[instruction, text] for text in texts]\n",
    "            return self.model.encode(paired_inputs, convert_to_numpy=True)\n",
    "        \n",
    "        # GTE Models - No instructions required\n",
    "        elif 'gte' in self.model_name:\n",
    "            return self.model.encode(texts, convert_to_numpy=True)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a6e2285b-8b8f-495e-b541-ee42a1146903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== E5 Embedding ===\n",
      "Modèle E5 chargé: e5-large-v2\n",
      "E5 embedding shape: (778, 1024)\n",
      "Sample values: [ 0.00914955 -0.04589565  0.03256417  0.02174237 -0.01066862]...\n",
      "\n",
      "=== Instructor Embedding ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name hkunlp/instructor-xl. Creating a new one with mean pooling.\n",
      "WARNING:sentence_transformers.SentenceTransformer:`SentenceTransformer._target_device` has been deprecated, please use `SentenceTransformer.device` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle Instructor chargé: instructor-xl\n",
      "Instructor embedding shape: (1, 1024)\n",
      "Sample values: [ 0.00064827 -0.06732617  0.00796025 -0.0156702   0.16671199]...\n",
      "\n",
      "=== GTE Embedding ===\n",
      "Modèle GTE chargé: gte-base\n",
      "GTE embedding shape: (1, 768)\n",
      "Sample values: [ 0.01220494 -0.02685339  0.02171323  0.04435849  0.07564417]...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== E5 Embedding ===\")\n",
    "e5_embedder = InstructionEmbeddings(\"e5-large-v2\")\n",
    "embeddings = e5_embedder.encode(\n",
    "    paragraph_segments, \n",
    "    task_type=\"search\"\n",
    ")\n",
    "print(f\"E5 embedding shape: {embeddings.shape}\")  # (1, 1024)\n",
    "print(f\"Sample values: {embeddings[0][:5]}...\")\n",
    "\n",
    "print(\"\\n=== Instructor Embedding ===\")\n",
    "instructor_embedder = InstructionEmbeddings(\"instructor-xl\")\n",
    "embeddings = instructor_embedder.encode(\n",
    "    [\"Quantum computing principles\"], \n",
    "    instruction=\"Represent the scientific text for retrieval:\"\n",
    ")\n",
    "print(f\"Instructor embedding shape: {embeddings.shape}\")  # (1, 768)\n",
    "print(f\"Sample values: {embeddings[0][:5]}...\")\n",
    "\n",
    "print(\"\\n=== GTE Embedding ===\")\n",
    "gte_embedder = InstructionEmbeddings(\"gte-base\")\n",
    "embeddings = gte_embedder.encode(\n",
    "    [\"This is a general text embedding example\"]\n",
    ")\n",
    "print(f\"GTE embedding shape: {embeddings.shape}\")  # (1, 768)\n",
    "print(f\"Sample values: {embeddings[0][:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57576fec-7a99-4aa3-8056-5c60885ba552",
   "metadata": {},
   "source": [
    "# Document-level Embeddings (long context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62cf90-32ec-42e5-871c-d8234dc68088",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Document-level embeddings (long context) involve generating a single numerical vector representing an entire long document, which can contain several thousand tokens, rather than being limited to individual sentences or chunks. This approach relies on advanced transformer models designed to handle large contexts, such as Longformer, which uses sparse attention to efficiently process sequences up to 4,096 tokens or more, or variants of GPT embeddings with extended context windows (for example, architectures like xAI’s Grok or models like Qwen2.5 embeddings). The principle is based on capturing the overall semantics of the document by aggregating contextual information across its entire length, often through mechanisms such as pooling (mean, max, or attention-weighted) applied to intermediate layer outputs. For example, a 3,000-word report on climate change would be encoded into a single vector reflecting the main themes (like CO2 emissions or environmental policies), rather than being split into independent segments. These embeddings are particularly useful for tasks requiring a holistic understanding, such as document classification or semantic search over long texts in RAG systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Capturing global semantics**: These embeddings synthesize the entire content of a document, allowing the representation of themes or cross-sectional ideas that span multiple sections, such as the development of an argument in an essay or a multi-chapter analysis.\n",
    "2. **Efficiency for long documents**: Models like Longformer or Qwen2.5, with their extended context capabilities, eliminate the need to pre-segment documents, simplifying RAG pipelines and reducing context loss caused by cuts.\n",
    "3. **Improved relevance in RAG**: By providing a unified representation, these embeddings enable more precise search over entire documents, avoiding biases introduced by arbitrary segmentation and improving the quality of generated responses.\n",
    "4. **Adaptability to complex corpora**: They are particularly suited to narrative texts, technical reports, or books where relationships between distant parts are crucial, offering a robust solution for applications such as legal or scientific document analysis.\n",
    "5. **Reduced redundancy**: Unlike overlapping-chunk approaches, a single vector per document decreases the amount of data to index, optimizing resources in large knowledge bases.\n",
    "\n",
    "---\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **High computational cost**: Generating an embedding for a document of several thousand tokens requires significant resources (GPU, memory), especially with models like GPT embeddings or Longformer, making inference slow on large corpora.\n",
    "2. **Loss of granularity**: By producing a single vector, these embeddings may obscure local details or semantic variations within the document, which can be problematic for tasks requiring fine-grained analysis (e.g., extracting a specific quote).\n",
    "3. **Model quality dependence**: Performance depends on the model’s ability to generalize over long contexts. A poorly trained or domain-mismatched model may produce unrepresentative embeddings, especially for heterogeneous texts.\n",
    "4. **Hardware limitations**: Even with optimized architectures like Longformer, the maximum sequence length (often 4,096 or 8,192 tokens) may be insufficient for extremely long documents, requiring truncation or prior segmentation.\n",
    "5. **Integration complexity**: Implementing these embeddings in a RAG pipeline requires specific resource management and compatibility with existing infrastructures, adding a layer of difficulty compared to sentence- or chunk-level embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Resource optimization**: Use lightweight versions or techniques like quantization to reduce computational load while preserving embedding quality.\n",
    "* **Hybrid segmentation**: Combine with initial segmentation (e.g., by sections) to handle overly long documents, then apply document-level embeddings to the subunits.\n",
    "* **Domain-specific fine-tuning**: Adapt the model on a corpus of long documents in a specific domain (e.g., financial reports or scientific articles) to improve representativeness.\n",
    "* **Custom attention mechanisms**: Integrate attention mechanisms focused on key parts of the document (e.g., summaries or conclusions) to enhance embedding relevance.\n",
    "* **Evaluation**: Test embeddings with metrics such as retrieval accuracy in a RAG context or thematic coherence on benchmarks like LongBench to validate their effectiveness.\n",
    "\n",
    "This technique requires robust infrastructure and careful configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fed5a9a6-c3b6-4fb0-8ac8-55d4579d828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LongDocumentEmbedder:\n",
    "    def __init__(self, model_name: str = \"allenai/longformer-base-4096\"):\n",
    "        \"\"\"\n",
    "        Initialize a model for long-document embeddings\n",
    "        Available models:\n",
    "            * Longformer: `'allenai/longformer-base-4096'`\n",
    "            * GPT-NeoX: `'EleutherAI/gpt-neox-20b'`\n",
    "            * Qwen: `'Qwen/Qwen1.5-7B'`\n",
    "            * BGE-M3: `'BAAI/bge-m3'` (supports 8,192 tokens)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the pre-trained model and its corresponding tokenizer.\n",
    "        This internal method initializes the language model and tokenizer for generating \n",
    "        embeddings or performing other NLP tasks. It ensures that the model is ready for \n",
    "        inference and handles any necessary configuration, such as device placement \n",
    "        (CPU/GPU) or model-specific settings.\n",
    "    \n",
    "        Returns\n",
    "            None\n",
    "    \n",
    "        Notes\n",
    "            This is an internal method and is typically called automatically during the \n",
    "            initialization of the embedding class.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Special configuration for some models\n",
    "        if \"longformer\" in self.model_name.lower():\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                attention_window=512,\n",
    "                gradient_checkpointing=True\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(self.model_name)\n",
    "            \n",
    "        print(f\"Model {self.model_name} successfully loaded!\")\n",
    "        print(f\"Maximum context length: {self.tokenizer.model_max_length} tokens\")\n",
    "            \n",
    "\n",
    "    def _chunk_text(self, text: str, chunk_size: int = 4000) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits a long text into smaller, manageable chunks for processing.\n",
    "\n",
    "        This function breaks the input text into segments of up to `chunk_size` tokens or characters, \n",
    "        depending on the implementation. It aims to create \"smart\" chunks, which means it tries \n",
    "        to avoid splitting sentences or paragraphs abruptly, preserving semantic coherence \n",
    "        within each chunk.\n",
    "    \n",
    "        Parameters\n",
    "            text : str\n",
    "                The input text string that needs to be divided into chunks.\n",
    "            chunk_size : int, optional (default=4000)\n",
    "                The maximum size of each chunk. The function will attempt to keep chunks below this \n",
    "                limit while maintaining logical boundaries in the text.\n",
    "    \n",
    "        Returns\n",
    "            List[str]\n",
    "                A list of text chunks, each as a string, ready for further processing \n",
    "                (e.g., embeddings generation or document analysis).\n",
    "    \n",
    "        Example\n",
    "            >>> text = \"This is a long document that needs to be split into smaller parts...\"\n",
    "            >>> chunks = self._chunk_text(text, chunk_size=1000)\n",
    "            >>> len(chunks)\n",
    "            3\n",
    "        \"\"\"\n",
    "        # Simple breakdown by sentences (to be improved as needed)\n",
    "        sentences = text.split('. ')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(current_chunk) + len(sentence) < chunk_size:\n",
    "                current_chunk += sentence + \". \"\n",
    "            else:\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence + \". \"\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def encode(self, text: str, pooling: str = \"mean\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Encodes a given text into a fixed-size numerical vector (embedding).\n",
    "        This function generates a vector representation of the input text using a \n",
    "        pre-trained language model. The output embedding captures the semantic meaning \n",
    "        of the text and can be used for tasks such as semantic search, clustering, \n",
    "        or downstream NLP applications.\n",
    "    \n",
    "        Parameters\n",
    "            text : str\n",
    "                The input text string to be converted into an embedding.\n",
    "            pooling : str, optional (default=\"mean\")\n",
    "                The pooling strategy to aggregate token-level embeddings into a single vector:\n",
    "                - \"mean\": average of all token embeddings\n",
    "                - \"max\": maximum value of each dimension across token embeddings\n",
    "                - \"attention\": attention-weighted aggregation (if supported by the model)\n",
    "    \n",
    "        Returns\n",
    "            np.ndarray\n",
    "                A 1-dimensional NumPy array representing the text embedding.\n",
    "        \n",
    "        Example\n",
    "            >>> embedding = encoder.encode(\"This is a sample document.\", pooling=\"mean\")\n",
    "            >>> embedding.shape\n",
    "            (768,)\n",
    "        \"\"\"\n",
    "        # Trim text if necessary\n",
    "        chunks = self._chunk_text(text)\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            inputs = self.tokenizer(\n",
    "                chunk,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512#self.tokenizer.model_max_length\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            # Retrieving hidden states\n",
    "            last_hidden = outputs.last_hidden_state.squeeze(0)\n",
    "            \n",
    "            # Pooling\n",
    "            if pooling == \"mean\":\n",
    "                chunk_embedding = torch.mean(last_hidden, dim=0)\n",
    "            elif pooling == \"max\":\n",
    "                chunk_embedding, _ = torch.max(last_hidden, dim=0)\n",
    "            elif pooling == \"cls\":\n",
    "                chunk_embedding = last_hidden[0]\n",
    "            else:\n",
    "                raise ValueError(\"Méthode de pooling non supportée\")\n",
    "            \n",
    "            all_embeddings.append(chunk_embedding.numpy())\n",
    "        \n",
    "        # Combining chunk embeddings\n",
    "        doc_embedding = np.mean(all_embeddings, axis=0)\n",
    "        \n",
    "        # L2 Normalization\n",
    "        doc_embedding = doc_embedding / np.linalg.norm(doc_embedding)\n",
    "        \n",
    "        return doc_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8a2dace6-3768-4bb0-aea4-cba9b3cdca8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Longformer (4096 tokens) ===\n",
      "Modèle allenai/longformer-base-4096 chargé avec succès!\n",
      "Longueur maximale de contexte: 1000000000000000019884624838656 tokens\n",
      "Dimension de l'embedding: (768,)\n",
      "Valeurs exemple: [-0.00249754 -0.00301268  0.01257276  0.00879303 -0.01029909]...\n",
      "\n",
      "=== BGE-M3 (8192 tokens) ===\n",
      "Modèle BAAI/bge-m3 chargé avec succès!\n",
      "Longueur maximale de contexte: 8192 tokens\n",
      "Dimension de l'embedding: (1024,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Longformer (4096 tokens) ===\")\n",
    "long_embedder = LongDocumentEmbedder(\"allenai/longformer-base-4096\")\n",
    "\n",
    "long_text = paragraph_segments[100][0:50]   # Your long document\n",
    "embedding = long_embedder.encode(long_text)\n",
    "print(f\"Dimension de l'embedding: {embedding.shape}\")\n",
    "print(f\"Valeurs exemple: {embedding[:5]}...\")\n",
    "\n",
    "print(\"\\n=== BGE-M3 (8192 tokens) ===\")\n",
    "bge_embedder = LongDocumentEmbedder(\"BAAI/bge-m3\")\n",
    "embedding = bge_embedder.encode(long_text)\n",
    "print(f\"Embedding dimension: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2511bbf-caf3-41cb-802a-e8d96574cec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c6422-69f1-4833-be3a-ec935ce9d932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
