{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9a2545-e696-4256-935f-e96a02480373",
   "metadata": {},
   "source": [
    "# <center>Panongbene Sawadogo</center>\n",
    "\n",
    "📩 **Contact** : amet1900@gmail.com\n",
    "\n",
    "🌐 **Linkedin** : https://www.linkedin.com/in/panongbene-jean-mohamed-sawadogo-33234a168/\n",
    "\n",
    "🗓️ **Dernière modification** : 16 August 2025\n",
    "\n",
    "# <center>**Document Segmentation Techniques for RAG Implementation**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a900e76-20c7-4cc7-8ff0-b996b8abea65",
   "metadata": {},
   "source": [
    "> In this document, I implement and compare different document segmentation techniques into *chunks*, in an optimal way, to build a Retrieval-Augmented Generation (RAG) system.\n",
    "> The goal is to explore how to divide text into coherent and usable units in order to:\n",
    ">\n",
    "> * improve information retrieval accuracy,\n",
    "> * optimize indexing,\n",
    "> * and provide the model with relevant context for generating high-quality responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de957d8-2b1b-4ec3-b890-3989c22aa987",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c856d1-28d9-4656-9052-258b80d411af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "#!pip install PyPDF2\n",
    "#!pip install matplotlib\n",
    "#!pip install bitsandbytes\n",
    "#!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aa463f7-9d33-40f6-b515-f9ddd533b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import PyPDF2\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "import matplotlib.pyplot as plt\n",
    "from rich.console import Console\n",
    "from dataclasses import dataclass\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from IPython.display import Markdown, display\n",
    "from typing import List, Dict, Tuple, Union, Callable, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, TextIteratorStreamer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45667854-abe4-42c3-91f2-18d315d1cb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if not already done (only once)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except LookupError:\n",
    "    nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fa74a-ad15-4516-be86-c01f74bc814e",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f64adc5-1276-46d4-9803-8aea4e3d693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs/SuiteNumerique.md', 'r', encoding='utf-8') as f:\n",
    "    sample_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fceaa55-c492-4cb7-9abc-24c31f3ae928",
   "metadata": {},
   "source": [
    "# Fixed-length segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43008a2f-c00a-42ee-a48b-095e6d7a5fca",
   "metadata": {},
   "source": [
    "#### *Principle*\n",
    "\n",
    "This technique consists of dividing the text into blocks of fixed size, without taking into account specific technical or grammatical criteria. For example, a block length is predefined (in number of tokens or characters), and the text is split into equal segments corresponding to that length. This approach does not consider the natural boundaries of the text, such as sentences, paragraphs, or syntactic structures. For instance, if a block length of 100 characters is chosen, the text will be cut every 100 characters, regardless of whether this happens in the middle of a word or an idea.\n",
    "\n",
    "---\n",
    "\n",
    "#### *Advantages*\n",
    "\n",
    "The main advantage of this method lies in its simplicity of implementation, fast execution, and high flexibility. It does not require complex linguistic analysis or advanced preprocessing, which makes it easy to integrate into various text processing pipelines. Moreover, it is robust and adaptable to different types of content, whether source code, English text, Arabic, or any other language or format. This universality makes it particularly useful in scenarios where data diversity is high and where standardization is preferable to customization.\n",
    "\n",
    "---\n",
    "\n",
    "#### *Disadvantages*\n",
    "\n",
    "However, this method has several significant limitations. The main issue is the risk of cutting an idea or sentence in the middle, which can harm the semantic coherence of the generated blocks. For example, a sentence like “The project was a success thanks to the team that worked hard” could be split between “The project was a success thanks to the team” and “that worked hard,” making each part difficult to interpret on its own. Another drawback is the possible need to add padding (e.g., with spaces or neutral characters) for the last block, which might not reach the predefined length, thus introducing noise or inconsistency in the processed data. Finally, this approach may lead to a loss of contextual information, especially for tasks that require a global understanding of the text, such as in Retrieval-Augmented Generation (RAG) systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0058cf5-c3e6-4da5-9080-8897609e0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_fixed_chunks(text, chunk_size, padding_char=\" \"):\n",
    "    \"\"\"\n",
    "    Splits a given text into fixed-size chunks.\n",
    "    This function divides the input text into consecutive blocks of a specified length (`chunk_size`).\n",
    "    If the last block is shorter than the desired length, it is padded with the specified character\n",
    "    (`padding_char`) until it reaches the required size.\n",
    "    Parameters:\n",
    "        text: The input text to split. Must not be empty.\n",
    "        chunk_size: The length of each chunk. Must be strictly positive.\n",
    "        padding_char: The character used to pad the last chunk if it is shorter\n",
    "                                      than `chunk_size`. Defaults to a space (\" \").\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks of equal length (`chunk_size`), except when no padding is required.\n",
    "    Raises:\n",
    "        ValueError: If the input text is empty or if `chunk_size` is not strictly positive.\n",
    "    Example:\n",
    "        >>> split_text_into_fixed_chunks(\"HelloWorld\", 4)\n",
    "        ['Hell', 'oWor', 'ld  ']\n",
    "\n",
    "        >>> split_text_into_fixed_chunks(\"abcdef\", 3, \"_\")\n",
    "        ['abc', 'def']\n",
    "    \"\"\"\n",
    "\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"The text must not be empty\")\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"The chunk size must be strictly positive\")\n",
    "\n",
    "    chunks = []\n",
    "    text_length = len(text)\n",
    "\n",
    "    for i in range(0, text_length, chunk_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # The chunk size must be strictly positive\n",
    "    if chunks and len(chunks[-1]) < chunk_size:\n",
    "        padding_needed = chunk_size - len(chunks[-1])\n",
    "        chunks[-1] += padding_char * padding_needed\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762f9f4-b10b-42ca-8597-11776aae039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple\n",
    "split_text_into_fixed_chunks(sample_text, 100, padding_char=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ad2a8-2d48-49e0-a1b0-5bdb2a0f9cfc",
   "metadata": {},
   "source": [
    "# Sentence or Paragraph Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ccb4a-4bc3-4665-beed-b8c0282571be",
   "metadata": {},
   "source": [
    "#### *Principle*\n",
    "\n",
    "Sentence or paragraph-based segmentation involves dividing a text into coherent units based on natural boundaries defined by linguistic or typographic structure. In sentence segmentation, the text is split at terminal points (e.g., periods, exclamation marks, or question marks), taking grammatical rules into account to identify complete units of meaning. For paragraph segmentation, the text is divided according to line breaks or separation markers (such as double line breaks), grouping multiple sentences into logical blocks that represent a single idea or subtopic. This method often relies on natural language processing (NLP) tools, such as NLTK, SpaCy, or regular expressions, to automatically detect these boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "#### *Advantages*\n",
    "\n",
    "1. **Preservation of semantic coherence**: By respecting natural boundaries, this method maintains the integrity of ideas and the relationships between elements within a sentence or paragraph. For example, a sentence like “Despite the challenges, the team achieved its goals through dedication” remains intact, allowing for clear understanding.\n",
    "2. **Improved embedding quality**: Embedding models (such as BERT or SentenceTransformers) perform better with units that capture complete context. A sentence or paragraph provides a richer vector representation than isolated small fragments.\n",
    "3. **Noise reduction**: Unlike arbitrary splits (e.g., fixed-size chunks), this approach avoids cutting through the middle of an idea, minimizing the risk of misinterpretation or loss of information.\n",
    "4. **Adaptability to complex tasks**: In applications like RAG, where retrieving relevant text is crucial, well-defined segments enable more precise searches and coherent responses, especially for queries requiring contextual understanding.\n",
    "5. **Ease of implementation with existing tools**: Libraries like SpaCy or NLTK offer ready-to-use functions (e.g., `sent_tokenize` or `para_tokenize`) that automate segmentation, reducing the need for custom development.\n",
    "\n",
    "---\n",
    "\n",
    "#### *Disadvantages*\n",
    "\n",
    "1. **Dependence on detection errors**: Automatic segmentation can fail when faced with poorly structured texts, such as incomplete sentences, abbreviations (e.g., “Dr.”), or typographical errors (e.g., a missing period). For example, “Dr. Smith spoke.” could be misinterpreted as two sentences.\n",
    "2. **Inconsistency in segment lengths**: Sentences or paragraphs naturally vary in length (e.g., one sentence may have 5 words, another 20), which can be problematic for models with token limits or requiring uniformity in processed data.\n",
    "3. **Increased complexity**: Compared to fixed-size splitting, this method requires NLP tools or custom rules, increasing implementation complexity and software dependencies.\n",
    "4. **Loss of context between segments**: While internal coherence is preserved, relationships between adjacent sentences or paragraphs can be broken if no overlap or linking mechanism is applied. For instance, an idea developed over two consecutive paragraphs might be fragmented.\n",
    "5. **Lower performance on very large texts**: For massive documents, paragraph segmentation can produce units that are too long, exceeding the processing limits of some models (e.g., 512 tokens for BERT), requiring additional splitting or truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a223a89c-feb7-4375-a67a-8c8ae6bebd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmenter_texte(text: str, mode: str = \"sentences\", chunk_size: int = 1, language: str = \"french\" ) -> List[str]:\n",
    "    \"\"\"\n",
    "    Segments a text into chunks of sentences or paragraphs.\n",
    "    Parameters:\n",
    "        text: Text to segment\n",
    "        mode: \"sentences\" or \"paragraphs\"\n",
    "        chunk_size: Number of elements per chunk\n",
    "        language: Language used for sentence tokenization\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    Raises:\n",
    "        ValueError: If parameters are invalid\n",
    "    \"\"\"\n",
    "    # Validation of entries\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        raise ValueError(\"Text must not be empty\")\n",
    "    \n",
    "    if mode not in {\"phrases\", \"paragraphes\"}:\n",
    "        raise ValueError(\"Mode must be 'sentences' or 'paragraphs'\")\n",
    "    \n",
    "    if not isinstance(chunk_size, int) or chunk_size <= 0:\n",
    "        raise ValueError(\"The chunk size must be a positive integer\")\n",
    "\n",
    "    # Text cleaning\n",
    "    text = text.strip()\n",
    "    \n",
    "    if mode == \"phrases\":\n",
    "        # Sentence Segmentation with NLTK\n",
    "        elements = sent_tokenize(text, language=language)\n",
    "    else:\n",
    "        # Paragraph segmentation\n",
    "        elements = [p for p in text.split('\\n') if p.strip()]\n",
    "    \n",
    "    # Creating chunks\n",
    "    chunks = []\n",
    "    for i in range(0, len(elements), chunk_size):\n",
    "        chunk = elements[i:i + chunk_size]\n",
    "        \n",
    "        # Join with space for sentences, line break for paragraphs\n",
    "        separator = \" \" if mode == \"phrases\" else \"\\n\"\n",
    "        chunks.append(separator.join(chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497e9250-7e1c-4f42-968c-cf391a5495de",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_segments = segmenter_texte(sample_text, \"phrases\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ac2b8-ed7a-40f0-b644-034ba9095380",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b101d-2b5e-4dc1-b2c1-e8b259e1e720",
   "metadata": {},
   "source": [
    "# Hierarchical segmentation (by headings and subheadings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec401c22-7abf-4b36-b193-cef6d5b61e6a",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Hierarchical segmentation by headings and subheadings involves dividing a text according to its document structure, by identifying and leveraging heading levels (e.g., H1, H2, H3 in an HTML document, or conventions like \"1. Introduction\", \"1.1 Objectives\") and subheadings that organize content into sections and subsections. This method relies on recognizing typographical or syntactic markers (such as numbering, separator lines, or specific tags) to delimit text blocks associated with each hierarchical level. For example, in a technical manual, a heading like “2. Installation” might be followed by a subheading “2.1 Prerequisites,” each corresponding to a distinct semantic unit. This approach can be automated using text processing tools (e.g., HTML parsing with BeautifulSoup or pattern detection with regular expressions) or implemented manually based on the known structure of the document.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Effective for structured documents**: This method excels at processing formal documents such as reports, technical manuals, academic articles, or books, where headings and subheadings serve as clear anchors for organizing information. For instance, a user manual can be segmented into sections like “Configuration” and “Troubleshooting” for quick navigation.\n",
    "2. **Preserves contextual structure**: By respecting the hierarchy, each segment retains its context within the overall structure, facilitating comprehension and retrieval of specific information. This is particularly useful in search systems or retrieval-augmented generation (RAG) applications.\n",
    "3. **Ease of indexing and navigation**: Hierarchical segments can be indexed with their corresponding headings, allowing targeted search (e.g., “see section 3.2”) and intuitive navigation, improving user experience in digital interfaces.\n",
    "4. **Reduces unnecessary overlaps**: Unlike methods based on fixed sizes or arbitrary overlaps, this segmentation aligns with the author’s intent, minimizing cuts in the middle of ideas or themes.\n",
    "5. **Adaptable to updates**: For evolving documents (such as wikis or knowledge bases), hierarchical segmentation allows adding or modifying sections without disrupting the overall structure, facilitating maintenance.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Less suited for unstructured texts**: This method is ineffective for texts lacking headings or explicit structure, such as emails, conversation transcripts, or informal blog posts. For example, continuous text without tags or numbering cannot be reliably segmented.\n",
    "2. **Dependent on structure quality**: If headings or subheadings are poorly defined, inconsistent (e.g., missing levels or numbering errors), or absent in parts of the document, segmentation may produce incomplete or misaligned segments.\n",
    "3. **Implementation complexity**: Automation requires specific tools (e.g., HTML parsing, pattern detection with regex) or manual annotation, increasing workload compared to simpler methods like fixed-size splitting.\n",
    "4. **Risk of unbalanced segments**: Sections defined by headings can vary greatly in length (e.g., a 50-word subsection versus a 500-word one), which can be problematic for models with processing limits (like token limits in AI models).\n",
    "5. **Loss of fine-grained context**: Although the hierarchy is preserved, transitions between subsections or details within a segment may be overlooked, especially if segmentation does not consider overlaps or intra-section relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Automated detection**: Use libraries like BeautifulSoup (for HTML) or regular expressions to robustly identify headings and subheadings, even across different formats (Markdown, Word, PDF).\n",
    "* **Contextual overlap**: Include overlap between sections (e.g., include the end of one subsection in the next) to preserve logical transitions.\n",
    "* **Length normalization**: Apply a maximum length limit or split overly long sections into subsegments while respecting hierarchy.\n",
    "* **Manual validation**: For critical documents, combine automatic segmentation with human review to correct detection errors.\n",
    "* **Evaluation**: Test this method using metrics such as semantic coverage or retrieval accuracy in a RAG context.\n",
    "\n",
    "This technique is particularly suitable for domains where document structure is an advantage, such as technical documentation or knowledge management, but requires careful preparation for less organized texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a0f881-0a24-4c50-9900-0f9e1b313cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DocumentSection:\n",
    "    title: str\n",
    "    level: int\n",
    "    content: str\n",
    "    subsections: List['DocumentSection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e48b440-0a6d-43a0-9927-ef7416bceed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation_hiérarchique(\n",
    "    texte: str,\n",
    "    motifs_titres: List[Tuple[str, int]] = [\n",
    "        (r'^\\#\\s(.+)$', 1),         # Markdown # Title\n",
    "        (r'^\\#\\#\\s(.+)$', 2),       # Markdown ## Sub-Title\n",
    "        (r'^\\#\\#\\#\\s(.+)$', 3),     # Markdown ### Sub-sous-Title\n",
    "        (r'^I\\.\\s(.+)$', 1),        # I. Title\n",
    "        (r'^[A-Z]\\.\\s(.+)$', 2),    # A. Sub-Title\n",
    "        (r'^\\d+\\.\\s(.+)$', 3),      # 1. Sub-Sub-Title\n",
    "        (r'^[A-Z][^\\.]+:$', 1),     # TITLE:\n",
    "        (r'^[A-Z][^\\.]+:$', 2)      # Sub-TITLE:\n",
    "    ]\n",
    ") -> DocumentSection:\n",
    "    \"\"\"\n",
    "    Segments a text hierarchically based on its structure.\n",
    "    Parametrs:\n",
    "        texte: Text to be segmented\n",
    "        motifs_titres: List of tuples (regex, level) to identify headings    \n",
    "    Returns:\n",
    "        Hierarchical structure of the document with sections and subsections\n",
    "    \"\"\"\n",
    "    if not texte.strip():\n",
    "        raise ValueError(\"Text must not be empty\")\n",
    "    \n",
    "    # Pretreatment\n",
    "    lignes = [ligne.strip() for ligne in texte.split('\\n') if ligne.strip()]\n",
    "    racine = DocumentSection(title=\"Racine\", level=0, content=\"\", subsections=[])\n",
    "    pile = [racine]\n",
    "    \n",
    "    for ligne in lignes:\n",
    "        titre, niveau = detecter_titre(ligne, motifs_titres)\n",
    "        if titre:\n",
    "            # Create a new section\n",
    "            nouvelle_section = DocumentSection(\n",
    "                title=titre,\n",
    "                level=niveau,\n",
    "                content=\"\",\n",
    "                subsections=[]\n",
    "            )            \n",
    "            # Move up the stack to the appropriate parent\n",
    "            while len(pile) > 1 and pile[-1].level >= niveau:\n",
    "                pile.pop()\n",
    "            \n",
    "            # Add to parent section\n",
    "            pile[-1].subsections.append(nouvelle_section)\n",
    "            pile.append(nouvelle_section)\n",
    "        else:\n",
    "            # Add content to the current section\n",
    "            if pile[-1].content:\n",
    "                pile[-1].content += \"\\n\" + ligne\n",
    "            else:\n",
    "                pile[-1].content = ligne\n",
    "    \n",
    "    return racine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7cc1c3-88de-4909-a2a4-2a1b2c382620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detecter_titre(ligne: str, motifs: List[Tuple[str, int]]) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "    Detects if a line is a heading and returns its text and level.\n",
    "    \"\"\"\n",
    "    for motif, niveau in motifs:\n",
    "        match = re.match(motif, ligne)\n",
    "        if match:\n",
    "            titre = match.group(1) if len(match.groups()) > 0 else ligne\n",
    "            return (titre.strip(), niveau)\n",
    "    return (None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6821c033-70c7-4658-b06a-1c07dab8b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_structure(section: DocumentSection, indent: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Recursively displays the document structure.\n",
    "    \"\"\"\n",
    "    if section.title != \"Racine\" or section.subsections:\n",
    "        print(\" \" * indent + f\"{section.title} (niveau {section.level})\")\n",
    "    for subsection in section.subsections:\n",
    "        afficher_structure(subsection, indent + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40364c-3a10-48a6-ac87-ed593ab37018",
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = segmentation_hiérarchique(sample_text)\n",
    "print(\"=== Hierarchical structure ===\")\n",
    "afficher_structure(structure)\n",
    "\n",
    "print(\"\\n=== Contents of a section ===\")\n",
    "print(structure.subsections[0].subsections[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ac0aa-fea0-4f72-ba74-35e5276587b2",
   "metadata": {},
   "source": [
    "# Overlapping Segmentation (Overlapping Chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03037780-0a61-4593-b6d1-5724e086ec69",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Overlapping segmentation (overlapping chunks) involves dividing a text into blocks (chunks) of a predefined size, where each chunk partially overlaps with the previous or next chunk to preserve context between segments. This method requires defining a chunk size (e.g., 200 characters or 50 tokens) and an overlap length (e.g., 50 characters or 10 tokens), which represents the portion shared between two consecutive chunks. For example, if a text is split into chunks of 100 characters with a 20-character overlap, the second chunk will start 80 characters after the beginning of the first, including the last 20 characters of the first chunk. This approach can be applied at the character, word, or token level and is often combined with other segmentation techniques (e.g., by sentence) to optimize contextual coherence. Implementation can be done using simple loops or text-processing tools like NLTK, adjusting slicing indices according to the overlap.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Reduced context loss**: Overlapping maintains logical transitions and semantic relationships between chunks, preventing abrupt cuts in the middle of ideas. For example, if a sentence like “The project succeeded thanks to the team who worked hard” is split, the overlap can include “the team” in the next chunk, aiding comprehension.\n",
    "2. **Improved embedding quality**: Embedding models (e.g., BERT or SentenceTransformers) benefit from this contextual continuity, producing richer and more coherent vector representations, which is crucial for tasks such as semantic search or text generation.\n",
    "3. **Flexibility in RAG applications**: In retrieval-augmented systems, overlapping chunks allow for more relevant information retrieval, even for queries that depend on context spanning multiple segments.\n",
    "4. **Adaptable to varied texts**: This method works well with texts of different lengths and structures (narrative, technical, conversational), as long as the overlap size is adjusted according to semantic density.\n",
    "5. **Easy optimization**: The overlap percentage can be finely tuned (e.g., 10%, 20% of chunk size) to balance coherence and resource usage, making the method adaptable to specific constraints.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Increased total corpus size**: Overlapping generates redundant chunks, increasing the total number of segments and, consequently, the amount of data to store and process. For example, a 1000-character text split into 200-character chunks with a 50-character overlap can nearly double the number of chunks compared to non-overlapping segmentation.\n",
    "2. **Higher computational cost**: Indexing and searching a larger corpus require more resources (CPU time, memory), which can be an issue for large datasets or low-power systems.\n",
    "3. **Risk of excessive redundancy**: If the overlap is too large (e.g., 50% of chunk size), much of the content becomes duplicated, diluting the diversity of retrieved information and unnecessarily increasing processing load.\n",
    "4. **Complexity in setting boundaries**: Choosing the optimal chunk and overlap size requires prior analysis of the text, and poor calibration can either fail to preserve context or overload the system.\n",
    "5. **Challenges with short texts**: In very short documents, a large overlap can make chunks nearly identical, reducing segmentation efficiency and increasing noise in the processed data.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Dynamic overlap adjustment**: Use semantic analysis (e.g., with NLP models) to adapt overlap length based on information density or contextual transitions.\n",
    "* **Redundancy filtering**: Apply deduplication or similarity thresholding (e.g., using cosine similarity) to remove overly similar chunks, reducing corpus size.\n",
    "* **Batch optimization**: Process chunks in batches with variable overlap sizes to balance performance and coherence, especially for large text volumes.\n",
    "* **Integration with other methods**: Combine this segmentation with sentence or paragraph splitting to align overlaps with natural boundaries, improving semantic quality.\n",
    "* **Evaluation**: Measure the impact of overlap with metrics such as retrieval precision or contextual coherence in a RAG context.\n",
    "\n",
    "This technique is particularly useful in scenarios where preserving context is critical, such as long-document analysis or conversational AI systems, but it must be carefully calibrated to avoid unnecessary overload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36413b0a-0663-4a26-8d8d-dd9dd321c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_chunks(text: str,chunk_size: int = 5,overlap_size: int = 2,mode: str = \"sentences\",lang: str = \"french\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Segments a text with overlapping chunks.\n",
    "    Parameters:\n",
    "        text: Text to segment\n",
    "        chunk_size: Number of elements per chunk\n",
    "        overlap_size: Number of overlapping elements\n",
    "        mode: \"sentences\", \"tokens\", or \"paragraphs\"\n",
    "        lang: Language for tokenization\n",
    "    Returns:\n",
    "        List of overlapping chunks\n",
    "    Raises:\n",
    "        ValueError: If the parameters are invalid\n",
    "    \"\"\"\n",
    "    # Validation of parameters\n",
    "    if not text.strip():\n",
    "        raise ValueError(\"Text must not be empty\")\n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(\"chunk_size must be positive\")\n",
    "    if overlap_size >= chunk_size or overlap_size < 0:\n",
    "        raise ValueError(\"overlap_size must be less than chunk_size and positive\")\n",
    "    \n",
    "    # Tokenization by mode\n",
    "    if mode == \"sentences\":\n",
    "        elements = sent_tokenize(text, language=lang)\n",
    "    elif mode == \"paragraphs\":\n",
    "        elements = [p for p in text.split('\\n') if p.strip()]\n",
    "    elif mode == \"tokens\":\n",
    "        elements = re.findall(r'\\w+|\\S', text)  # Tokenisation simple\n",
    "    else:\n",
    "        raise ValueError(\"Mode doit être 'sentences', 'paragraphs' ou 'tokens'\")\n",
    "    \n",
    "    # Size Check\n",
    "    if len(elements) <= chunk_size:\n",
    "        return [\" \".join(elements)]\n",
    "    \n",
    "    # Generating chunks with overlap\n",
    "    chunks = []\n",
    "    step = chunk_size - overlap_size\n",
    "    for i in range(0, len(elements), step):\n",
    "        chunk = elements[i:i + chunk_size]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        \n",
    "        # Stop if you reach the end\n",
    "        if i + chunk_size >= len(elements):\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81baaddb-41b7-49ff-aa98-a9ea39d15d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs/SuiteNumerique.md', 'r', encoding='utf-8') as f:\n",
    "    sample_text = f.read()\n",
    "    \n",
    "print(\"=== Chevauchement de phrases ===\")\n",
    "chunks = overlapping_chunks(sample_text, chunk_size=3, overlap_size=1, mode=\"sentences\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk[:80]}...\")\n",
    "\n",
    "print(\"\\n=== Chevauchement de paragraphes ===\")\n",
    "chunks = overlapping_chunks(sample_text, chunk_size=2, overlap_size=1, mode=\"paragraphs\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk[:80]}...\")\n",
    "\n",
    "print(\"\\n=== Chevauchement de tokens ===\")\n",
    "chunks = overlapping_chunks(\"Ceci est un exemple de tokenisation.\", chunk_size=4, overlap_size=2, mode=\"tokens\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb36d54-416d-4b8d-a2e8-473596b928ea",
   "metadata": {},
   "source": [
    "# Semantic-based Segmentation (Semantic Chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1a503-5558-455c-b068-e6a786b9ba2d",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Semantic-based segmentation (semantic chunking) involves dividing a text into segments based on changes in topic or theme, rather than on syntactic criteria or fixed sizes. This method leverages natural language processing (NLP) models, such as embedding models (e.g., BERT, SentenceTransformers) to convert text into vector representations, followed by clustering algorithms (like K-means or DBSCAN) or similarity threshold detection to identify semantic transitions. The process begins by splitting the text into preliminary units (e.g., sentences or small blocks) and then evaluating semantic coherence between these units. When a significant topic change is detected (e.g., a drop in cosine similarity between two blocks), a new segmentation is created. For example, in a text shifting from \"Birds migrate in winter\" to \"Sustainable construction techniques,\" semantic chunking would create two distinct chunks reflecting the different themes, even if the sentences are contiguous.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Alignment with complete ideas**: Each chunk represents a coherent semantic unit, capturing a specific topic or idea. This ensures that segments do not cut off a thought midway, improving readability and comprehension.\n",
    "2. **Improved relevance in RAG**: In retrieval-augmented systems, this segmentation allows retrieving chunks that are more relevant to a given query, as they align with specific concepts rather than arbitrary boundaries. For example, a query on \"bird migration\" will retrieve only the relevant chunk without contextual noise.\n",
    "3. **Adaptability to different writing styles**: This method works with texts of various natures (narrative, technical, conversational) because it relies on meaning rather than a predefined structure, making its application universal.\n",
    "4. **Optimization of embeddings**: By grouping units that share semantic similarity, embedding models produce more consistent and accurate representations, enhancing search or classification performance.\n",
    "5. **Support for thematic analysis**: This segmentation facilitates extraction of main themes in large corpora, useful for document summarization or theme-based sentiment analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Requires complex NLP processing**: This method relies on embedding models (often resource-intensive) and clustering algorithms, increasing implementation complexity compared to simple segmentation methods (by fixed size or sentences). For example, using BERT may require GPU access to process large volumes of text.\n",
    "2. **High computational cost**: Calculating embeddings for each text unit, followed by similarity analysis or clustering, can be slow, especially for large documents, increasing processing time and hardware requirements.\n",
    "3. **Dependence on model quality**: The segmentation accuracy depends on the performance of the NLP model used. A poorly trained or domain-inappropriate model (e.g., a general model on medical text) can produce incoherent segments or misdetect topic changes.\n",
    "4. **Difficult parameter tuning**: Defining similarity thresholds or clustering parameters (e.g., number of clusters or maximum distance) requires manual calibration or experimental validation, which can be error-prone or subjective.\n",
    "5. **Limitations with ambiguous texts**: In texts where topic transitions are gradual or implicit (e.g., smooth narratives), semantic chunking may produce overly fragmented chunks or merge distinct ideas, affecting coherence.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Resource optimization**: Use lightweight models (e.g., DistilBERT) or precomputed embeddings to reduce computational load while maintaining good semantic performance.\n",
    "* **Dynamic threshold adjustment**: Implement adaptive detection of topic changes based on local semantic density rather than a fixed threshold to better capture transitions.\n",
    "* **Text preprocessing**: Clean and normalize the text (remove noise, correct errors) before segmentation to improve embedding quality.\n",
    "* **Hybrid validation**: Combine semantic segmentation with methods based on natural boundaries (e.g., sentences) to refine results and reduce errors.\n",
    "* **Evaluation**: Test segmentation using metrics like semantic coherence (e.g., ROUGE scores) or retrieval accuracy in a RAG context to fine-tune parameters.\n",
    "\n",
    "This technique is especially suitable for applications where semantic understanding is critical, such as complex document analysis or conversational AI systems, but it requires robust infrastructure and NLP expertise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d992817-bddc-4d07-8233-aeafc8f4fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunker:\n",
    "    \"\"\"\n",
    "    Semantics-based text segmenter that detects topic changes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', threshold: float = 0.85, window_size: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize the semantic segmenter.\n",
    "        Parameters:\n",
    "          model_name: Name of the SentenceTransformer model to use\n",
    "          threshold: Similarity threshold for detecting topic changes\n",
    "          window_size: Size of the sliding window for calculating similarity\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.threshold = threshold\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def embed_sentences(self, sentences: List[str]) -> np.ndarray:\n",
    "        \"\"\"Converts sentences to embeddings.\"\"\"\n",
    "        return self.model.encode(sentences, convert_to_tensor=False)\n",
    "    \n",
    "    def calculate_similarities(self, embeddings: np.ndarray) -> List[float]:\n",
    "        \"\"\"Calculates similarities between consecutive sentences.\"\"\"\n",
    "        similarities = []\n",
    "        for i in range(len(embeddings) - 1):\n",
    "            sim = cosine_similarity(\n",
    "                embeddings[i].reshape(1, -1),\n",
    "                embeddings[i + 1].reshape(1, -1)\n",
    "            )[0][0]\n",
    "            similarities.append(sim)\n",
    "        return similarities\n",
    "    \n",
    "    def find_chunk_boundaries(self, similarities: List[float]) -> List[int]:\n",
    "        \"\"\"Find chunk boundaries based on similarities\"\"\"\n",
    "        boundaries = []\n",
    "        window = []\n",
    "        \n",
    "        for i, sim in enumerate(similarities):\n",
    "            window.append(sim)\n",
    "            if len(window) > self.window_size:\n",
    "                window.pop(0)\n",
    "            \n",
    "            # Si la moyenne de la fenêtre est en dessous du seuil, c'est une frontière\n",
    "            if len(window) == self.window_size and np.mean(window) < self.threshold:\n",
    "                boundaries.append(i + 1)  # +1 car on veut l'indice après le saut\n",
    "        \n",
    "        return boundaries\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"\n",
    "        Segments the text into semantic chunks.\n",
    "        Returns:\n",
    "            List of tuples (start_idx, end_idx, chunk_text)\n",
    "        \"\"\"\n",
    "        # Separation into sentences (simplistic - to be improved as needed)\n",
    "        sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "        if not sentences:\n",
    "            return []\n",
    "        \n",
    "        # Sentence embeddings\n",
    "        embeddings = self.embed_sentences(sentences)\n",
    "        \n",
    "        # Calculating similarities between consecutive sentences\n",
    "        similarities = self.calculate_similarities(embeddings)\n",
    "        if not similarities:\n",
    "            return [(0, len(text), text)]\n",
    "        \n",
    "        # Finding the limits of chunks\n",
    "        boundaries = self.find_chunk_boundaries(similarities)\n",
    "        \n",
    "        # Rebuild chunks from boundaries\n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        # Find the start/end positions in the original text\n",
    "        sentence_offsets = [0]\n",
    "        current_pos = 0\n",
    "        for s in sentences:\n",
    "            current_pos += len(s) + 1  # +1 pour le point\n",
    "            sentence_offsets.append(current_pos)\n",
    "        \n",
    "        for boundary in boundaries:\n",
    "            end_idx = boundary\n",
    "            chunk_sentences = sentences[start_idx:end_idx]\n",
    "            chunk_start = sentence_offsets[start_idx]\n",
    "            chunk_end = sentence_offsets[end_idx] if end_idx < len(sentence_offsets) else len(text)\n",
    "            chunk_text = text[chunk_start:chunk_end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append((chunk_start, chunk_end, chunk_text))\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        # Add the last chunk\n",
    "        if start_idx < len(sentences):\n",
    "            chunk_start = sentence_offsets[start_idx]\n",
    "            chunk_text = text[chunk_start:].strip()\n",
    "            if chunk_text:\n",
    "                chunks.append((chunk_start, len(text), chunk_text))\n",
    "        \n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74a051-c7db-4507-87c8-a8b0acfff21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du chunker sémantique\n",
    "chunker = SemanticChunker(threshold=0.8)\n",
    "\n",
    "# Segmentation du texte\n",
    "chunks = chunker.chunk_text(sample_text)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Texte segmenté en chunks sémantiques:\")\n",
    "for i, (start, end, chunk) in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i} (positions {start}-{end}):\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b206a70-02b0-4d10-aea1-576f1c89c4d3",
   "metadata": {},
   "source": [
    "# Multi-level segmentation (hierarchical semantic chunking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c6af16-d7e2-4b69-975b-acc61d448cf7",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Multi-level segmentation, also called hierarchical semantic chunking, combines document-structure-based segmentation (based on titles, subtitles, and sections) with internal semantic segmentation within these units. This approach relies on two complementary steps: first, an initial split is performed according to the document's explicit hierarchy (for example, by identifying titles like \"1. Introduction\" or \"2.1 Methodology\" using tools such as BeautifulSoup or regular expressions), creating high-level segments. Then, each segment is semantically subdivided based on topic or thematic changes within it, using natural language processing (NLP) models such as embedding models (e.g., BERT or SentenceTransformers) and clustering or similarity-detection algorithms (like K-means or cosine-thresholds). For instance, in a section titled \"2. Results,\" a semantic sub-segmentation could separate \"Data Analysis\" from \"Trend Interpretation,\" even if these topics fall under the same title, ensuring fine granularity while preserving the overall structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Balance between document structure and semantic relevance**: This method leverages the explicit hierarchy to organize segments at a macro level while applying semantic analysis to refine sub-segments, providing an optimal combination of structural clarity and thematic coherence.\n",
    "2. **Improved relevance in RAG**: Multi-level segments enable more precise retrieval in RAG systems, as they align units to specific topics while maintaining their context within the overall structure. For example, a query on \"trend interpretation\" will directly target the relevant sub-segment within the \"Results\" section.\n",
    "3. **Adaptability to various formats**: This approach works well with structured documents (reports, manuals) while accommodating more fluid internal sections, making it versatile across heterogeneous corpora.\n",
    "4. **Preservation of hierarchical context**: By keeping titles as metadata, this segmentation facilitates navigation and understanding, allowing users or models to follow the document's organizational logic.\n",
    "5. **Optimization for thematic analysis**: Combining the two methods allows extraction of themes at different levels (global via structure, local via semantics), which is valuable for tasks such as summarization or content classification.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **More complex pipeline to implement**: Combining structure-based segmentation (requiring parsing tools) and semantic segmentation (requiring NLP models and clustering algorithms) increases pipeline complexity, demanding technical expertise and careful integration.\n",
    "2. **High computational cost**: Internal semantic analysis, involving computing embeddings for each sub-segment and applying clustering, adds significant load, especially for large documents or massive corpora, increasing resource requirements (CPU/GPU, memory).\n",
    "3. **Dependence on the quality of both steps**: Accuracy depends on both correct title detection (sensitive to formatting errors) and semantic model performance (sensitive to training data or domain). Weakness in either step can compromise overall results.\n",
    "4. **Extended processing time**: The dual analysis (structural then semantic) lengthens processing time compared to simpler methods, which can be a limitation for real-time applications.\n",
    "5. **Risk of over-segmentation**: If semantic thresholds are too strict or the structure is misinterpreted, segmentation may produce excessive sub-segments, reducing indexing or retrieval efficiency and increasing noise in the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Resource optimization**: Use lightweight models (like DistilBERT) for semantic segmentation and efficient tools (like simple regex) for title detection, reducing computational load.\n",
    "* **Hybrid calibration**: Dynamically adjust semantic similarity thresholds based on section length or density to avoid over-segmentation.\n",
    "* **Structured preprocessing**: Normalize title formats (e.g., converting \"1.1\" and \"Section 1.1\" into a single standard) to improve initial detection.\n",
    "* **Integration of overlaps**: Add overlap between semantic sub-segments to preserve contextual transitions while respecting the hierarchy.\n",
    "* **Evaluation**: Test segmentation using metrics such as retrieval precision (in RAG) or thematic coherence (via scores like ROUGE), comparing results with univariate methods.\n",
    "\n",
    "This technique is particularly suited for complex documents requiring both clear organization and fine-grained content analysis, such as technical reports or knowledge bases, but it requires robust infrastructure and careful configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fa51079-0b2e-493f-b36c-cf61dee274a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkNode:\n",
    "    \"\"\"Represents a node in the chunk hierarchy\"\"\"\n",
    "    text: str\n",
    "    start: int\n",
    "    end: int\n",
    "    level: int\n",
    "    children: List['ChunkNode']\n",
    "    semantic_group: Optional[int] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae8f7501-0cfe-4eb9-a5a8-4ce47c928943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSemanticChunker:\n",
    "    \"\"\"\n",
    "    Hierarchical segmenter that combines structure and semantics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = 'all-MiniLM-L6-v2',\n",
    "                 semantic_threshold: float = 0.82,\n",
    "                 window_size: int = 3,\n",
    "                 heading_pattern: str = r'^(#+)\\s*(.*)$'):\n",
    "        \"\"\"\n",
    "        Initialize the hierarchical segmenter.\n",
    "        Parameters:\n",
    "            model_name: Embedding model to use\n",
    "            semantic_threshold: Threshold for semantic segmentation\n",
    "            window_size: Window size for similarity calculation\n",
    "            heading_pattern: Regex pattern to detect headings\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.semantic_threshold = semantic_threshold\n",
    "        self.window_size = window_size\n",
    "        self.heading_pattern = re.compile(heading_pattern, re.MULTILINE)\n",
    "    \n",
    "    def _detect_structure(self, text: str) -> List[Tuple[int, int, int, str]]:\n",
    "        \"\"\"\n",
    "        Detects the document structure (headings and sections).\n",
    "        Returns:\n",
    "            List of tuples `(start, end, level, text)`\n",
    "        \"\"\"\n",
    "        structure = []\n",
    "        last_pos = 0\n",
    "        \n",
    "        for match in self.heading_pattern.finditer(text):\n",
    "            # Text before current title\n",
    "            if match.start() > last_pos:\n",
    "                structure.append((last_pos, match.start(), 0, text[last_pos:match.start()].strip()))\n",
    "            \n",
    "            # The title itself\n",
    "            level = len(match.group(1))  # Number of # determines level\n",
    "            structure.append((match.start(), match.end(), level, match.group(2).strip()))\n",
    "            last_pos = match.end()\n",
    "        \n",
    "        # Add text after the last title\n",
    "        if last_pos < len(text):\n",
    "            structure.append((last_pos, len(text), 0, text[last_pos:].strip()))\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def _semantic_chunking(self, text: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Semantic segmentation of a text (similar to the previous implementation)\"\"\"\n",
    "        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "        if len(sentences) < 2:\n",
    "            return [(0, len(text), text)]\n",
    "        \n",
    "        embeddings = self.model.encode(sentences, convert_to_tensor=False)\n",
    "        similarities = []\n",
    "        \n",
    "        for i in range(len(embeddings) - 1):\n",
    "            sim = cosine_similarity(embeddings[i].reshape(1, -1), \n",
    "                                   embeddings[i + 1].reshape(1, -1))[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        boundaries = []\n",
    "        window = []\n",
    "        \n",
    "        for i, sim in enumerate(similarities):\n",
    "            window.append(sim)\n",
    "            if len(window) > self.window_size:\n",
    "                window.pop(0)\n",
    "            \n",
    "            if len(window) == self.window_size and np.mean(window) < self.semantic_threshold:\n",
    "                boundaries.append(i + 1)\n",
    "        \n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        sentence_offsets = [0]\n",
    "        current_pos = 0\n",
    "        \n",
    "        for s in sentences:\n",
    "            current_pos += len(s) + 1  # +1 pour l'espace après la phrase\n",
    "            sentence_offsets.append(current_pos)\n",
    "        \n",
    "        for boundary in boundaries:\n",
    "            end_idx = boundary\n",
    "            chunk_start = sentence_offsets[start_idx]\n",
    "            chunk_end = sentence_offsets[end_idx] if end_idx < len(sentence_offsets) else len(text)\n",
    "            chunk_text = text[chunk_start:chunk_end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append((chunk_start, chunk_end, chunk_text))\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk_start = sentence_offsets[start_idx]\n",
    "            chunk_text = text[chunk_start:].strip()\n",
    "            if chunk_text:\n",
    "                chunks.append((chunk_start, len(text), chunk_text))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _build_hierarchy(self, structure: List[Tuple[int, int, int, str]]) -> ChunkNode:\n",
    "        \"\"\"Builds the chunk hierarchy from the detected structure\"\"\"\n",
    "        root = ChunkNode(text=\"\", start=0, end=0, level=-1, children=[])\n",
    "        stack = [root]\n",
    "        \n",
    "        for start, end, level, text in structure:\n",
    "            node = ChunkNode(text=text, start=start, end=end, level=level, children=[])\n",
    "            \n",
    "            # Trouver le parent approprié dans la stack\n",
    "            while len(stack) > 1 and stack[-1].level >= level:\n",
    "                stack.pop()\n",
    "            \n",
    "            stack[-1].children.append(node)\n",
    "            stack.append(node)\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def _add_semantic_chunks(self, node: ChunkNode, text: str):\n",
    "        \"\"\"Adds semantic segmentation to the leaves of the hierarchy\"\"\"\n",
    "        if not node.children and len(node.text.split()) > 20:  # Seulement pour les nœuds textuels assez longs\n",
    "            semantic_chunks = self._semantic_chunking(node.text)\n",
    "            \n",
    "            for chunk_start, chunk_end, chunk_text in semantic_chunks:\n",
    "                # Ajuster les positions par rapport au texte complet\n",
    "                abs_start = node.start + chunk_start\n",
    "                abs_end = node.start + chunk_end\n",
    "                child = ChunkNode(text=chunk_text, start=abs_start, end=abs_end, \n",
    "                                level=node.level + 1, children=[])\n",
    "                node.children.append(child)\n",
    "        \n",
    "        for child in node.children:\n",
    "            self._add_semantic_chunks(child, text)\n",
    "    \n",
    "    def _assign_semantic_groups(self, node: ChunkNode, text: str, group_counter: int = 0):\n",
    "        \"\"\"Assigns semantic groups to chunks to identify common themes\"\"\"\n",
    "        if not node.children:\n",
    "            # Embedding seulement pour les feuilles\n",
    "            embedding = self.model.encode([node.text], convert_to_tensor=False)[0]\n",
    "            node.embedding = embedding\n",
    "            \n",
    "            # Trouver un groupe similaire existant ou en créer un nouveau\n",
    "            best_group = None\n",
    "            best_sim = -1\n",
    "            \n",
    "            # Browse existing groups (simplified implementation)\n",
    "            # In a real implementation, one would maintain a list of group centroids\n",
    "            if hasattr(self, '_group_centroids'):\n",
    "                for group_id, centroid in self._group_centroids.items():\n",
    "                    sim = cosine_similarity([embedding], [centroid])[0][0]\n",
    "                    if sim > self.semantic_threshold and sim > best_sim:\n",
    "                        best_sim = sim\n",
    "                        best_group = group_id\n",
    "            \n",
    "            if best_group is not None:\n",
    "                node.semantic_group = best_group\n",
    "                # Mettre à jour le centroïde\n",
    "                self._group_centroids[best_group] = (self._group_centroids[best_group] + embedding) / 2\n",
    "            else:\n",
    "                node.semantic_group = group_counter\n",
    "                if not hasattr(self, '_group_centroids'):\n",
    "                    self._group_centroids = {}\n",
    "                self._group_centroids[group_counter] = embedding\n",
    "                group_counter += 1\n",
    "        \n",
    "        for child in node.children:\n",
    "            group_counter = self._assign_semantic_groups(child, text, group_counter)\n",
    "        \n",
    "        return group_counter\n",
    "    \n",
    "    def chunk_document(self, text: str) -> ChunkNode:\n",
    "        \"\"\"Segments a document hierarchically and semantically\"\"\"\n",
    "        # Step 1: Structure detection (headings/sections)\n",
    "        structure = self._detect_structure(text)\n",
    "        \n",
    "        # Step 2: Building the Hierarchy\n",
    "        root = self._build_hierarchy(structure)\n",
    "        \n",
    "        # Step 3: Semantic segmentation of textual content\n",
    "        self._add_semantic_chunks(root, text)\n",
    "        \n",
    "        # Step 4: Semantic grouping of chunks\n",
    "        self._assign_semantic_groups(root, text)\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def print_chunks(self, node: ChunkNode, indent: int = 0):\n",
    "        \"\"\"Displays the chunk hierarchy (for visualization)\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        if node.level >= 0:\n",
    "            group_info = f\" [Groupe {node.semantic_group}]\" if node.semantic_group is not None else \"\"\n",
    "            print(f\"{prefix}Niveau {node.level}: {node.text[:60]}...{group_info} (pos: {node.start}-{node.end})\")\n",
    "        \n",
    "        for child in node.children:\n",
    "            self.print_chunks(child, indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99ef3120-21c2-4042-bed7-d6ff1f113f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_ = sample_text[0:1625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d740f3a8-6981-46e4-abea-e0ef7b890b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du chunker hiérarchique\n",
    "chunker = HierarchicalSemanticChunker(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    semantic_threshold=0.8,\n",
    "    window_size=2\n",
    ")\n",
    "\n",
    "# Segmentation du document\n",
    "document_tree = chunker.chunk_document(sample_text_)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Structure hiérarchique du document avec segmentation sémantique:\")\n",
    "chunker.print_chunks(document_tree)\n",
    "\n",
    "# Exemple d'accès aux chunks\n",
    "def get_all_chunks(node):\n",
    "    chunks = []\n",
    "    if node.text and not node.children:  # Feuille avec contenu\n",
    "        chunks.append((node.start, node.end, node.text, node.level, node.semantic_group))\n",
    "    for child in node.children:\n",
    "        chunks.extend(get_all_chunks(child))\n",
    "    return chunks\n",
    "\n",
    "all_chunks = get_all_chunks(document_tree)\n",
    "print(\"\\nListe plate de tous les chunks:\")\n",
    "for chunk in all_chunks:\n",
    "    print(f\"Lvl {chunk[3]} - Groupe {chunk[4]}: {chunk[2][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267d6d3d-0b79-416b-a953-26203c7ae02d",
   "metadata": {},
   "source": [
    "# Segmentation based on sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a134f76c-28a1-4918-a85a-5ea7a5e2edd0",
   "metadata": {},
   "source": [
    "### Principle\n",
    "\n",
    "Sliding window–based segmentation involves processing a text continuously using a fixed-size window that moves incrementally by X tokens (or characters) at each step to generate chunks. The window size defines the length of each segment (e.g., 100 tokens), while the increment (X) determines the offset between the starts of consecutive windows (e.g., 50 tokens). This method treats the text as a linear sequence, naturally producing overlaps between chunks based on the difference between the window size and the increment. For example, with a 200-character window and a 100-character increment, the first chunk covers characters 1–200, the second 101–300, and so on. This approach can be implemented with simple loops or tokenization tools (like NLTK or SpaCy) and is particularly suited for texts where local context is more important than semantic or structural boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages\n",
    "\n",
    "1. **Precise size control**: Segmentation allows exact definition of chunk length (e.g., 50, 100, or 200 tokens), providing uniformity that facilitates adaptation to AI model processing limits (such as token limits in BERT or GPT).\n",
    "2. **Precise context control**: By adjusting the increment, the degree of overlap between chunks can be regulated, ensuring context is preserved in a controlled manner. For example, a smaller increment (e.g., 25 tokens on a 100-token window) maximizes overlap and continuity.\n",
    "3. **Flexible application**: This method works effectively on various types of text (narrative, technical, conversational) without requiring prior semantic or structural analysis, making it quick to implement in processing pipelines.\n",
    "4. **Optimization for large corpora**: The systematic nature of sliding allows automated and scalable processing, ideal for indexing large volumes of data in RAG systems or search engines.\n",
    "5. **Ease of tuning**: Parameters (window size and increment) can be adjusted according to specific needs, such as text density or performance constraints, without changing the core logic.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **Unnecessary duplication if misconfigured**: If the increment is too small relative to the window size (e.g., 10 tokens on a 100-token window), a large portion of content is repeated across chunks, increasing redundancy and corpus size without adding semantic value.\n",
    "2. **Loss of semantic coherence**: Since segmentation ignores natural boundaries (sentences, topics), it may split ongoing ideas, making chunks less interpretable on their own. For example, a sentence like “The project succeeded thanks to the team” could be divided between two chunks, diluting its meaning.\n",
    "3. **Higher computational cost with small increments**: A small increment generates more overlapping chunks, increasing processing load for indexing and search, especially on long texts.\n",
    "4. **Sensitivity to parameter tuning**: Performance depends heavily on the choice of window size and increment. Poor configuration (e.g., a window too large for dense text) can either lose context or overload the system with redundant data.\n",
    "5. **Less suited for structured texts**: This method ignores hierarchies (titles, sections) or semantic transitions, making it less effective for documents where structure is an asset, like manuals or reports.\n",
    "\n",
    "---\n",
    "\n",
    "### Improvements and Suggestions\n",
    "\n",
    "* **Dynamic increment adjustment**: Adapt the increment based on semantic density or sentence length to minimize duplication while preserving context.\n",
    "* **Integration of natural boundaries**: Combine the sliding window with sentence or paragraph detection to align cuts with coherent units, reducing abrupt splits.\n",
    "* **Redundancy filtering**: Apply similarity-based deduplication (e.g., with a cosine threshold) to remove overly similar chunks, optimizing corpus size.\n",
    "* **Batch optimization**: Process text in larger blocks with internal sliding windows, reducing computational overhead for very long documents.\n",
    "* **Evaluation**: Test segmentation using metrics like contextual coverage or retrieval accuracy in a RAG context to fine-tune parameters.\n",
    "\n",
    "This technique is especially suited for scenarios requiring granular control over context and size, such as indexing massive corpora or training AI models, but careful calibration is needed to avoid drawbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ee8475-920d-45e4-aebf-aea8bf91fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ChunkNode:\n",
    "    \"\"\"Represents a node in the chunk hierarchy\"\"\"\n",
    "    text: str\n",
    "    start: int\n",
    "    end: int\n",
    "    level: int\n",
    "    children: List['ChunkNode']\n",
    "    semantic_group: Optional[int] = None\n",
    "\n",
    "class HierarchicalSemanticChunker:\n",
    "    \"\"\"\n",
    "    Hierarchical segmenter that combines structure and semantics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = 'all-MiniLM-L6-v2',\n",
    "                 semantic_threshold: float = 0.82,\n",
    "                 window_size: int = 3,\n",
    "                 heading_pattern: str = r'^(#+)\\s*(.*)$'):\n",
    "        \"\"\"\n",
    "        Initialize the hierarchical segmenter.\n",
    "        Parameters:\n",
    "             model_name: Embedding model to use\n",
    "             semantic_threshold: Threshold for semantic segmentation\n",
    "             window_size: Window size for similarity\n",
    "             heading_pattern: Regex pattern to detect headings\n",
    "\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.semantic_threshold = semantic_threshold\n",
    "        self.window_size = window_size\n",
    "        self.heading_pattern = re.compile(heading_pattern, re.MULTILINE)\n",
    "    \n",
    "    def _detect_structure(self, text: str) -> List[Tuple[int, int, int, str]]:\n",
    "        \"\"\"\n",
    "        Detects the document structure (headings and sections).\n",
    "        Returns:\n",
    "             List of tuples (start, end, level, text)\n",
    "        \"\"\"\n",
    "        structure = []\n",
    "        last_pos = 0\n",
    "        \n",
    "        for match in self.heading_pattern.finditer(text):\n",
    "            # Text before current title\n",
    "            if match.start() > last_pos:\n",
    "                structure.append((last_pos, match.start(), 0, text[last_pos:match.start()].strip()))\n",
    "            \n",
    "            # The title itself\n",
    "            level = len(match.group(1))  # Number of # determines level\n",
    "            structure.append((match.start(), match.end(), level, match.group(2).strip()))\n",
    "            last_pos = match.end()\n",
    "        \n",
    "        # Add text after the last title\n",
    "        if last_pos < len(text):\n",
    "            structure.append((last_pos, len(text), 0, text[last_pos:].strip()))\n",
    "        \n",
    "        return structure\n",
    "    \n",
    "    def _semantic_chunking(self, text: str) -> List[Tuple[int, int, str]]:\n",
    "        \"\"\"Semantic segmentation of a text (similar to the previous implementation)\"\"\"\n",
    "        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', text) if s.strip()]\n",
    "        if len(sentences) < 2:\n",
    "            return [(0, len(text), text)]\n",
    "        \n",
    "        embeddings = self.model.encode(sentences, convert_to_tensor=False)\n",
    "        similarities = []\n",
    "        \n",
    "        for i in range(len(embeddings) - 1):\n",
    "            sim = cosine_similarity(embeddings[i].reshape(1, -1), \n",
    "                                   embeddings[i + 1].reshape(1, -1))[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        boundaries = []\n",
    "        window = []\n",
    "        \n",
    "        for i, sim in enumerate(similarities):\n",
    "            window.append(sim)\n",
    "            if len(window) > self.window_size:\n",
    "                window.pop(0)\n",
    "            \n",
    "            if len(window) == self.window_size and np.mean(window) < self.semantic_threshold:\n",
    "                boundaries.append(i + 1)\n",
    "        \n",
    "        chunks = []\n",
    "        start_idx = 0\n",
    "        sentence_offsets = [0]\n",
    "        current_pos = 0\n",
    "        \n",
    "        for s in sentences:\n",
    "            current_pos += len(s) + 1  # +1 pour l'espace après la phrase\n",
    "            sentence_offsets.append(current_pos)\n",
    "        \n",
    "        for boundary in boundaries:\n",
    "            end_idx = boundary\n",
    "            chunk_start = sentence_offsets[start_idx]\n",
    "            chunk_end = sentence_offsets[end_idx] if end_idx < len(sentence_offsets) else len(text)\n",
    "            chunk_text = text[chunk_start:chunk_end].strip()\n",
    "            \n",
    "            if chunk_text:\n",
    "                chunks.append((chunk_start, chunk_end, chunk_text))\n",
    "            start_idx = end_idx\n",
    "        \n",
    "        if start_idx < len(sentences):\n",
    "            chunk_start = sentence_offsets[start_idx]\n",
    "            chunk_text = text[chunk_start:].strip()\n",
    "            if chunk_text:\n",
    "                chunks.append((chunk_start, len(text), chunk_text))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _build_hierarchy(self, structure: List[Tuple[int, int, int, str]]) -> ChunkNode:\n",
    "        \"\"\"Builds the chunk hierarchy from the detected structure\"\"\"\n",
    "        root = ChunkNode(text=\"\", start=0, end=0, level=-1, children=[])\n",
    "        stack = [root]\n",
    "        \n",
    "        for start, end, level, text in structure:\n",
    "            node = ChunkNode(text=text, start=start, end=end, level=level, children=[])\n",
    "            \n",
    "            # Find the appropriate parent in the stack\n",
    "            while len(stack) > 1 and stack[-1].level >= level:\n",
    "                stack.pop()\n",
    "            \n",
    "            stack[-1].children.append(node)\n",
    "            stack.append(node)\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def _add_semantic_chunks(self, node: ChunkNode, text: str):\n",
    "        \"\"\"Adds semantic segmentation to the leaves of the hierarchy\"\"\"\n",
    "        if not node.children and len(node.text.split()) > 20:  # Only for fairly long text nodes\n",
    "            semantic_chunks = self._semantic_chunking(node.text)\n",
    "            \n",
    "            for chunk_start, chunk_end, chunk_text in semantic_chunks:\n",
    "                # Adjust positions relative to full text\n",
    "                abs_start = node.start + chunk_start\n",
    "                abs_end = node.start + chunk_end\n",
    "                child = ChunkNode(text=chunk_text, start=abs_start, end=abs_end, \n",
    "                                level=node.level + 1, children=[])\n",
    "                node.children.append(child)\n",
    "        \n",
    "        for child in node.children:\n",
    "            self._add_semantic_chunks(child, text)\n",
    "    \n",
    "    def _assign_semantic_groups(self, node: ChunkNode, text: str, group_counter: int = 0):\n",
    "        \"\"\"Assigns semantic groups to chunks to identify common themes.\"\"\"\n",
    "        if not node.children:\n",
    "            # Embedding only for sheets\n",
    "            embedding = self.model.encode([node.text], convert_to_tensor=False)[0]\n",
    "            node.embedding = embedding\n",
    "            \n",
    "            # Find an existing similar group or create a new one\n",
    "            best_group = None\n",
    "            best_sim = -1\n",
    "            \n",
    "            # Iterate through existing groups (simplified implementation)\n",
    "            # In a real implementation, we would maintain a list of group centroids\n",
    "            if hasattr(self, '_group_centroids'):\n",
    "                for group_id, centroid in self._group_centroids.items():\n",
    "                    sim = cosine_similarity([embedding], [centroid])[0][0]\n",
    "                    if sim > self.semantic_threshold and sim > best_sim:\n",
    "                        best_sim = sim\n",
    "                        best_group = group_id\n",
    "            \n",
    "            if best_group is not None:\n",
    "                node.semantic_group = best_group\n",
    "                # Update the centroid\n",
    "                self._group_centroids[best_group] = (self._group_centroids[best_group] + embedding) / 2\n",
    "            else:\n",
    "                node.semantic_group = group_counter\n",
    "                if not hasattr(self, '_group_centroids'):\n",
    "                    self._group_centroids = {}\n",
    "                self._group_centroids[group_counter] = embedding\n",
    "                group_counter += 1\n",
    "        \n",
    "        for child in node.children:\n",
    "            group_counter = self._assign_semantic_groups(child, text, group_counter)\n",
    "        \n",
    "        return group_counter\n",
    "    \n",
    "    def chunk_document(self, text: str) -> ChunkNode:\n",
    "        \"\"\"Segments a document hierarchically and semantically\"\"\"\n",
    "        # Step 1: Structure detection (headings/sections)\n",
    "        structure = self._detect_structure(text)\n",
    "        \n",
    "        # Step 2: Building the Hierarchy\n",
    "        root = self._build_hierarchy(structure)\n",
    "        \n",
    "        # Step 3: Semantic segmentation of textual content\n",
    "        self._add_semantic_chunks(root, text)\n",
    "        \n",
    "        # Step 4: Semantic grouping of chunks\n",
    "        self._assign_semantic_groups(root, text)\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def print_chunks(self, node: ChunkNode, indent: int = 0):\n",
    "        \"\"\"Displays the chunk hierarchy (for visualization).\"\"\"\n",
    "        prefix = \"  \" * indent\n",
    "        if node.level >= 0:\n",
    "            group_info = f\" [Groupe {node.semantic_group}]\" if node.semantic_group is not None else \"\"\n",
    "            print(f\"{prefix}Niveau {node.level}: {node.text[:60]}...{group_info} (pos: {node.start}-{node.end})\")\n",
    "        \n",
    "        for child in node.children:\n",
    "            self.print_chunks(child, indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a576656-2d77-4e56-8e56-fa44867ae0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text_ = sample_text[0:1625]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197af99-f9c1-4514-9113-01445b08b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the hierarchical chunker\n",
    "chunker = HierarchicalSemanticChunker(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    semantic_threshold=0.8,\n",
    "    window_size=2\n",
    ")\n",
    "\n",
    "# Document segmentation\n",
    "document_tree = chunker.chunk_document(sample_text_)\n",
    "\n",
    "# Displaying resultss\n",
    "print(\"Hierarchical document structure with semantic segmentation:\")\n",
    "chunker.print_chunks(document_tree)\n",
    "\n",
    "# Example of accessing chunks\n",
    "def get_all_chunks(node):\n",
    "    chunks = []\n",
    "    if node.text and not node.children:  # Feuille avec contenu\n",
    "        chunks.append((node.start, node.end, node.text, node.level, node.semantic_group))\n",
    "    for child in node.children:\n",
    "        chunks.extend(get_all_chunks(child))\n",
    "    return chunks\n",
    "\n",
    "all_chunks = get_all_chunks(document_tree)\n",
    "print(\"\\nFlat list of all chunks:\")\n",
    "for chunk in all_chunks:\n",
    "    print(f\"Lvl {chunk[3]} - Groupe {chunk[4]}: {chunk[2][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377fd511-a4b6-458f-b3e5-4e97a9ab0533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
